/**
 * @brief Applies the Rectified Linear Unit activation function to the input matrix.
 *
 * The function maps all negative values in the input matrix to zero and all positive values to themselves.
 *
 * @param x Input matrix to apply the ReLU function to.
 * @param deriv Optional output matrix to store the derivative of the ReLU function.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
void ReLU<FloatType>::operator()(Matrix<FloatType> &x, Matrix<FloatType> *deriv) const{
  int dim = x.size(0);
  int batch_size = x.size(1);

  //f(x)_i = max(x_i, 0) = x_i  |  x_i > 0
  //                     = 0    |  x_i <= 0
  if(deriv == nullptr){
    autoView(x_v,x,DeviceReadWrite);

    accelerator_for2d(b,batch_size,i,dim,1,{
	if(x_v(i,b) <= 0.) x_v(i,b) = 0.;
    });
  }else{
    *deriv = Matrix<FloatType>(dim,batch_size);

    autoView(deriv_v, (*deriv), DeviceWrite);
    autoView(x_v,x,DeviceReadWrite);
    accelerator_for2d(b,batch_size,i,dim,1,{
	if(x_v(i,b) <= 0.){
	  x_v(i,b) = 0.;
	  deriv_v(i,b) = 0.;
	}else{
	  deriv_v(i,b) = 1.;
	}
      });
  }
}


/**
 * @brief Applies the Rectified Linear Unit (ReLU) activation function to the input tensor.
 *
 * The ReLU function maps all negative values to zero and all positive values to themselves.
 *
 * If the derivative tensor is not null, computes the derivative of the ReLU function with respect to the input tensor.
 *
 * @param[in,out] x Input tensor to apply the ReLU function to.
 * @param[out] deriv Derivative tensor of the ReLU function with respect to the input tensor, can be null.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
template<int Dim>
void ReLU<FloatType>::operator()(Tensor<FloatType,Dim> &x, Tensor<FloatType,Dim> *deriv) const{
  int const* dims = x.sizeArray();
  int batch_size = dims[x.dimension()-1];
  size_t size_other = 1; for(int i=0; i<x.dimension()-1; i++) size_other *= dims[i];
  
  //f(x)_i = max(x_i, 0) = x_i  |  x_i > 0
  //                     = 0    |  x_i <= 0
  if(deriv == nullptr){
    autoView(x_v,x,DeviceReadWrite);

    accelerator_for2d(b,batch_size,i,size_other, 1,{
	FloatType &xx = *(x_v.data() + b+batch_size*i);	
	if(xx <= 0.) xx = 0.;
    });
  }else{
    *deriv = Tensor<FloatType,Dim>(dims);

    autoView(deriv_v, (*deriv), DeviceWrite);
    autoView(x_v,x,DeviceReadWrite);
    accelerator_for2d(b,batch_size,i,size_other, 1,{
	size_t off = b + batch_size*i;
	FloatType &xx = *(x_v.data() + off);
	FloatType &dd = *(deriv_v.data() + off);
	
	if(xx <= 0.){
	  xx = 0.;
	  dd = 0.;
	}else{
	  dd = 1.;
	}
      });
  }
}
