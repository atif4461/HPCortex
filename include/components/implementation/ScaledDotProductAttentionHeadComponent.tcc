/**
 * @brief Computes scaled dot product attention head component value.
 *
 * This function calculates the output of a scaled dot product attention mechanism,
 * which takes query, key, and value tensors as input and returns a tensor representing
 * the computed attention weights applied to the value tensor.
 *
 * @param Q Query tensor with shape (C, E, B).
 * @param K Key tensor with shape (C, E, B).
 * @param V Value tensor with shape (C, E, B).
 *
 * @return Computed attention output tensor.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
Tensor<FloatType,3> ScaledDotProductAttentionHeadComponent<FloatType>::value(const Tensor<FloatType,3> &Q, const Tensor<FloatType,3> &K, const Tensor<FloatType,3> &V){
  //Q(C,E,B) ,  K(C,E,B)  and V(C,E,B)
  assert(Q.size(1) == E && K.size(1) == E && V.size(1) == E);

  if(!setup){
    C = Q.size(0);
    B = Q.size(2);
    setup = true;
  }

  assert(Q.size(0) == C && K.size(0) == C && V.size(0)==C);
  assert(Q.size(2) == B && K.size(2) == B && V.size(2)==B);

  Tensor<FloatType,3> Qp = multWQ.value(Q);
  Tensor<FloatType,3> Kp = multWK.value(K);
  Tensor<FloatType,3> Vp = multWV.value(V);
  return attention.value(Qp,Kp,Vp);
}
/**
 * Computes derivatives of cost with respect to model parameters.
 *
 * @param[out] cost_deriv         Vector containing derivative of cost with respect to model parameters
 * @param[in]  off               Offset index for parameter vector
 * @param[in]  _dCost_by_dOut     Derivative of cost with respect to output tensor
 * @param[out] dCost_by_dQ        Derivative of cost with respect to query weights
 * @param[out] dCost_by_dK        Derivative of cost with respect to key weights
 * @param[out] dCost_by_dV        Derivative of cost with respect to value weights
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
void ScaledDotProductAttentionHeadComponent<FloatType>::deriv(Vector<FloatType> &cost_deriv, int off, Tensor<FloatType,3> &&_dCost_by_dOut, Tensor<FloatType,3> &dCost_by_dQ, Tensor<FloatType,3> &dCost_by_dK, Tensor<FloatType,3> &dCost_by_dV) const{ 
  Tensor<FloatType,3> dCost_by_dOut = std::move(_dCost_by_dOut);
  assert(dCost_by_dOut.size(0) == C && dCost_by_dOut.size(1) == d_v && dCost_by_dOut.size(2) == B);

  //Params are in order of W_Q, W_K, W_V
  int p = off;
    
  Tensor<FloatType,3> above_deriv_Qp, above_deriv_Kp, above_deriv_Vp;
  attention.deriv(std::move(dCost_by_dOut), above_deriv_Qp, above_deriv_Kp, above_deriv_Vp);
    
  Tensor<FloatType,3> layer_deriv_K, layer_deriv_Q, layer_deriv_V; //these are  dCost/dX_{ceb} through the different routes
  multWQ.deriv(cost_deriv, p, std::move(above_deriv_Qp), dCost_by_dQ);
  p += multWQ.nparams();
  multWK.deriv(cost_deriv, p, std::move(above_deriv_Kp), dCost_by_dK);
  p += multWK.nparams();
  multWV.deriv(cost_deriv, p, std::move(above_deriv_Vp), dCost_by_dV);
}

/**
 * Updates the attention head component with new parameters starting at offset off.
 * @param off Offset where the update starts in the parameter vector.
 * @param new_params New set of model parameters.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
void ScaledDotProductAttentionHeadComponent<FloatType>::update(int off, const Vector<FloatType> &new_params){
  int p = off;
  multWQ.update(p,new_params);
  p += multWQ.nparams();
  multWK.update(p,new_params);
  p += multWK.nparams();
  multWV.update(p,new_params);
}
  
/**
 * Performs a step operation for the scaled dot product attention head component.
 * @param off offset value
 * @param derivs vector of derivatives
 * @param eps epsilon value used in computation
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
void ScaledDotProductAttentionHeadComponent<FloatType>::step(int off, const Vector<FloatType> &derivs, FloatType eps){
  int p = off;
  multWQ.step(p,derivs,eps);
  p += multWQ.nparams();
  multWK.step(p,derivs,eps);
  p += multWK.nparams();
  multWV.step(p,derivs,eps);
}

//off measured from *end*, return new off
/**
 * Retrieves parameters for the attention head component and stores them in the provided vector.
 * @param[out] into Vector to store the retrieved parameters
 * @param[in] off Offset index to start storing parameters
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
void ScaledDotProductAttentionHeadComponent<FloatType>::getParams(Vector<FloatType> &into, int off){
  int p = off;
  multWQ.getParams(into,p);
  p += multWQ.nparams();
  multWK.getParams(into,p);
  p += multWK.nparams();
  multWV.getParams(into,p);
}
