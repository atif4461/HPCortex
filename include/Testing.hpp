#pragma once

#include <Tensors.hpp>
#include <random>

/**
 * @brief Checks if two floating point numbers are nearly equal within a relative tolerance.
 * @param a The first floating point number.
 * @param b The second floating point number.
 * @param rel_tol The relative tolerance.
 * @param reldiff_p Pointer to store the relative difference between a and b.
 * @return True if the absolute relative difference between a and b is less than or equal to rel_tol, false otherwise.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
bool near(FloatType a, FloatType b, FloatType rel_tol, FloatType *reldiff_p = nullptr){
  FloatType diff = a - b;
  FloatType avg = (a + b)/2.;
  FloatType reldiff;
  if(avg == 0.0){
    if(diff != 0.0) reldiff=1.0;
    else reldiff = 0.0;
  }else{
    reldiff = diff / avg;
  }
  if(reldiff_p)  *reldiff_p = reldiff;
  
  if(fabs(reldiff) > rel_tol) return false;
  else return true;
}


/**
 * @brief Checks if two vectors are nearly equal within a relative tolerance.
 * 
 * This function compares two vectors element-wise and returns true if all elements are close enough,
 * as determined by the specified relative tolerance.
 * 
 * @param a The first vector to compare.
 * @param b The second vector to compare.
 * @param rel_tol The relative tolerance used to determine closeness.
 * @param verbose If true, prints information about non-matching elements.
 * @return True if the vectors are nearly equal, false otherwise.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
bool near(const Vector<FloatType> &a,const Vector<FloatType> &b, FloatType rel_tol, bool verbose=false){
  if(a.size(0) != b.size(0)) return false;
  autoView(a_v,a,HostRead);
  autoView(b_v,b,HostRead);
  for(size_t i=0;i<a.size(0);i++){
    FloatType reldiff;
    bool nr = near(a_v(i),b_v(i),rel_tol,&reldiff);
    if(!nr){
      if(verbose) std::cout << i << " a:" << a_v(i) << " b:" << b_v(i) << " rel.diff:" << reldiff << std::endl;
      return false;
    }
  }
  return true;
}


/**
 * @brief Checks if two matrices are nearly equal within a relative tolerance.
 * @param a The first matrix to compare.
 * @param b The second matrix to compare.
 * @param rel_tol The relative tolerance for comparison.
 * @param verbose If true, prints detailed information about unequal elements.
 * @return True if the matrices are nearly equal, false otherwise.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
bool near(const Matrix<FloatType> &a,const Matrix<FloatType> &b, FloatType rel_tol, bool verbose=false){
  if(a.size(0) != b.size(0) || a.size(1) != b.size(1)) return false;
  autoView(a_v,a,HostRead);
  autoView(b_v,b,HostRead);
  for(size_t i=0;i<a.size(0);i++){
    for(size_t j=0;j<a.size(1);j++){
    
      FloatType reldiff;
      bool nr = near(a_v(i,j),b_v(i,j),rel_tol,&reldiff);
      if(!nr){
	if(verbose) std::cout << i << " " << j << " a:" << a_v(i,j) << " b:" << b_v(i,j) << " rel.diff:" << reldiff << std::endl;
	return false;
      }
    }
  }
  return true;
}


/**
 * @brief Checks if absolute difference between two floating point numbers is within tolerance
 * @param a first floating point number
 * @param b second floating point number
 * @param abs_tol absolute tolerance value
 * @param absdiff_p pointer to store absolute difference (optional)
 * @return true if absolute difference is within tolerance, false otherwise
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
bool abs_near(FloatType a, FloatType b, FloatType abs_tol, FloatType *absdiff_p = nullptr){
  FloatType absdiff = fabs(a - b);
  if(absdiff_p) *absdiff_p = absdiff;
  if(absdiff > abs_tol) return false;
  else return true;
}


/**
 * @brief Checks if two matrices are approximately equal within absolute tolerance.
 *
 * This function compares two matrices element-wise and returns true if all elements
 * are within the specified absolute tolerance of each other.
 *
 * @param a The first matrix to compare.
 * @param b The second matrix to compare.
 * @param abs_tol The absolute tolerance for comparison.
 * @param verbose If true, prints detailed information about mismatched elements.
 * @return True if the matrices are approximately equal, false otherwise.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
bool abs_near(const Matrix<FloatType> &a,const Matrix<FloatType> &b, FloatType abs_tol, bool verbose=false){
  if(a.size(0) != b.size(0) || a.size(1) != b.size(1)) return false;
  autoView(a_v,a,HostRead);
  autoView(b_v,b,HostRead);
  for(size_t i=0;i<a.size(0);i++){
    for(size_t j=0;j<a.size(1);j++){
    
      FloatType absdiff;
      bool nr = abs_near(a_v(i,j),b_v(i,j),abs_tol,&absdiff);
      if(!nr){
	if(verbose) std::cout << i << " " << j << " a:" << a_v(i,j) << " b:" << b_v(i,j) << " abs.diff:" << absdiff << std::endl;
	return false;
      }
    }
  }
  return true;
}

/**
 * Checks if two tensors are approximately equal within absolute tolerance.
 *
 * @param[in] a first tensor to compare
 * @param[in] b second tensor to compare
 * @param[in] abs_tol absolute tolerance for comparison
 * @param[in] verbose flag to enable detailed output in case of mismatch
 * @return true if tensors are approximately equal, false otherwise
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, int Dim>
bool abs_near(const Tensor<FloatType,Dim> &a,const Tensor<FloatType,Dim> &b, FloatType abs_tol, bool verbose=false){
  int const* a_sz = a.sizeArray();
  int const* b_sz = b.sizeArray();
  for(int d=0;d<Dim;d++)
    if(a_sz[d] != b_sz[d]){
      if(verbose) std::cout << "Dimension mismatch on idx " << d << " a.size(idx)=" << a_sz[d] << " b.size(idx)=" << b_sz[d] << std::endl;
      return false;
    }
  
  autoView(a_v,a,HostRead);
  autoView(b_v,b,HostRead);
  for(size_t v = 0; v < a_v.data_len(); v++){
    FloatType absdiff;
    FloatType aa = a_v.data()[v];
    FloatType bb = b_v.data()[v];
    
    bool nr = abs_near(aa,bb,abs_tol,&absdiff);
    if(!nr){
      if(verbose){     
	int coord[Dim];    
	tensorOffsetUnmap<Dim>(coord,a_sz,v);
	for(int d=0;d<Dim;d++)
	  std::cout << coord[d] << " ";
	std::cout << "a:" << aa << " b:" << bb << " abs.diff:" << absdiff << std::endl;
      }
      return false;
    }
  }
  return true;
}

/**
 * Checks whether two tensors are element-wise equal.
 *
 * @param[in] a The first tensor to compare.
 * @param[in] b The second tensor to compare.
 * @param[in] verbose If true, prints detailed information about unequal elements.
 * @return True if all elements of the two tensors are equal, false otherwise.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, int Dim>
bool equal(const Tensor<FloatType,Dim> &a,const Tensor<FloatType,Dim> &b, bool verbose=false){
  int const* a_sz = a.sizeArray();
  int const* b_sz = b.sizeArray();
  for(int d=0;d<Dim;d++) if(a_sz[d] != b_sz[d]) return false;
  
  autoView(a_v,a,HostRead);
  autoView(b_v,b,HostRead);
  for(size_t v = 0; v < a_v.data_len(); v++){
    FloatType aa = a_v.data()[v];
    FloatType bb = b_v.data()[v];
    
    bool eq = aa == bb;
    if(!eq){
      if(verbose){     
	int coord[Dim];    
	tensorOffsetUnmap<Dim>(coord,a_sz,v);
	for(int d=0;d<Dim;d++)
	  std::cout << coord[d] << " ";
	std::cout << "a:" << aa << " b:" << bb << " abs.diff:" << aa-bb << std::endl;
      }
      return false;
    }
  }
  return true;
}

/**
 * @brief Initializes a matrix with random values within a uniform distribution.
 *
 * This function populates a given matrix with random floating point numbers 
 * between -1.0 and 1.0 using a uniform real distribution and a random number generator.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, typename RNG>
void random(Matrix<FloatType> &m, RNG &rng){
  if(m.data_len() == 0) return;
  std::uniform_real_distribution<FloatType> dist(-1.0, 1.0);
  autoView(m_v,m,HostWrite);
  for(int i=0;i<m.size(0);i++)
    for(int j=0;j<m.size(1);j++)
      m_v(i,j) = dist(rng);
}
/**
 * @brief Initializes a vector with random values within a specified range.
 *
 * This function populates a given vector with random floating point numbers
 * uniformly distributed between -1.0 and 1.0 using a provided random number generator.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, typename RNG>
void random(Vector<FloatType> &m, RNG &rng){
  if(m.data_len() == 0) return;
  std::uniform_real_distribution<FloatType> dist(-1.0, 1.0);
  autoView(m_v,m,HostWrite);
  for(int i=0;i<m.size(0);i++)
    m_v(i) = dist(rng);
}    
/**
 * @brief Fills a tensor with random values within a specified range
 * 
 * This function populates a tensor with random floating point numbers 
 * uniformly distributed between -1.0 and 1.0 using a provided random number generator.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, int Dim, typename RNG>
void random(Tensor<FloatType,Dim> &m, RNG &rng){
  if(m.data_len() == 0) return;
  std::uniform_real_distribution<FloatType> dist(-1.0, 1.0);
  autoView(m_v,m,HostWrite);
  int const* dims = m.sizeArray();
  size_t sz = tensorSize<Dim>(dims);
  for(size_t i=0; i<sz; i++){
    int coord[Dim];
    tensorOffsetUnmap<Dim>(coord, dims, i);
    m_v(coord) = dist(rng);
  }
}


/**
 * @brief Benchmarks the execution time of an operation with warm-up iterations.
 *
 * This function measures the average execution time and standard deviation of an operation,
 * taking into account a specified number of warm-up iterations to stabilize performance.
 *
 * @param[out] mean The calculated average execution time.
 * @param[out] std The calculated standard deviation of execution times.
 * @param[in] nrpt The number of repetitions to perform the operation for measurement.
 * @param[in] nwarmup The number of warm-up iterations before measuring execution times.
 * @param[in] op The operation to be executed and measured.
 * @param[in] preop An optional preparatory operation to execute before each repetition.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename Op, typename PreOp>
void benchmark(double &mean, double &std, int nrpt, int nwarmup, const Op &op, const PreOp &preop){
  auto t = now();
  for(int i=0;i<nwarmup;i++){
    preop();
    op();
  }
  double twarm = since(t);
  //std::cout << "Warmup " << twarm << std::endl;
  
  mean = std = 0.;
  for(int i=0;i<nrpt;i++){
    preop();
    t = now();
    op();
    double dt = since(t);
    mean += dt;
    std += dt*dt;

    //std::cout << i << " " << dt  << std::endl;
  }
  mean /= nrpt;
  std = sqrt( std/nrpt - mean*mean ); 
}

//A simple cost model for easy evaluation of derivatives : cost = \sum_i c_i * out_i   for linearized index i
/**
 * @brief Calculates the cost value by performing dot product operation between two input vectors.
 *
 * This function takes a vector c and a tensor v as inputs, performs element-wise multiplication 
 * and sums up the results to obtain the final cost value.
 *
 * @param c Input vector
 * @param v Input tensor
 * @return Calculated cost value
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename TensType>
typename TensType::FloatType testCost(const Vector<typename TensType::FloatType> &c, const TensType &v){
  typename TensType::FloatType out = 0.;
  doHost2(c,v,{
      for(size_t i=0;i<c.size(0);i++)
	out += v_v.data()[i] * c_v(i);
    });
  return out;
}

//Test the derivative of the model is implemented correctly using discrete derivative. Works for any model with tensor input/output of arbitrary dimension. It only assumes value() and nparams() are correct.
//model: the ML model (sans cost function)
//in_sizes, out_sizes : the tensor dimensions of the input and output
//delta : the shift used to evaluate the discrete derivatives
/**
 * @brief Tests the derivative calculations of a given model.
 *
 * This function checks the correctness of the model's update, step, getParams,
 * value, and derivative functions by comparing numerical approximations with
 * analytical results.
 *
 * @param model The model to be tested.
 * @param in_sizes The sizes of the input data.
 * @param out_sizes The sizes of the output data.
 * @param delta The small change used for numerical differentiation (default is 1e-4).
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename ModelType>
void testDeriv(ModelType &model, int* in_sizes, int* out_sizes, typename ModelType::FloatType delta = typename ModelType::FloatType(1e-4)){
  typedef LAYEROUTPUTTYPE(ModelType) OutputType;
  typedef typename ModelType::InputType InputType;
  typedef typename ModelType::FloatType FloatType;
  
  constexpr int OutDim = OutputType::Dimension;
  constexpr int InDim = InputType::Dimension;

  std::mt19937 rng(1987);

  //Check basic functionality
  int nparam = model.nparams(); //assumed correct
  Vector<FloatType> base_params(nparam);
  random(base_params, rng);

  model.update(0, base_params);
  {
    Vector<FloatType> testp(nparam);
    model.getParams(testp,0);
    assert(equal(testp,base_params,true));
  }
  {
    Vector<FloatType> shifts(nparam);
    random(shifts, rng);
    model.step(0,shifts,0.33);
    
    Vector<FloatType> pexpect = base_params - 0.33*shifts;
    Vector<FloatType> pgot(nparam);
    model.getParams(pgot,0);
    assert(abs_near(pexpect, pgot, 1e-4, true));
    
    model.update(0, base_params);
  }

  //Check derivatives  
  size_t vout = 1;
  for(int d=0;d<OutDim;d++)
    vout *= out_sizes[d];
  
  //let  cost = \sum_i c_i * out_i
  //above_deriv = dcost/dout_i = c_i
  Vector<FloatType> c(vout);
  random(c,rng);

  InputType in_base(in_sizes);
  random(in_base, rng);
  
  OutputType val_base = model.value(in_base);
  Vector<FloatType> pderiv_got(nparam,0.);

  OutputType above_deriv(out_sizes);
  doHost2(above_deriv, c, {
      for(size_t i=0;i<vout;i++)
	above_deriv_v.data()[i] = c_v(i);
    });
  InputType inderiv_got;
  model.deriv(pderiv_got,0,std::move(above_deriv),&inderiv_got);
 
  //Test parameter derivs
  //param_deriv = \sum_i dcost/dout_i  dout_i/dparam_j  =  \sum_i c_i dout_i/dparam_j
  FloatType cost_base = testCost(c, val_base);
  
  for(int j=0;j<nparam;j++){
    Vector<FloatType> pup(base_params);
    doHost(pup, { pup_v(j) += delta; });
    model.update(0,pup);
    
    OutputType vup = model.value(in_base);
    FloatType new_cost = testCost(c, vup);

    FloatType der = (new_cost - cost_base)/delta;
    doHost(pderiv_got, {
	std::cout << "Cost deriv wrt param " << j << " got " << pderiv_got_v(j) << " expect " << der << std::endl;
	assert(abs_near(der, pderiv_got_v(j), 1e-4)); 
      });
  }

  //Test layer deriv
  //layer_deriv_j = \sum_i dcost/dout_i dout_i/din_j
  model.update(0,base_params);

  size_t vin =  1;
  for(int d=0;d<InDim;d++)
    vin *= in_sizes[d];

  for(size_t j=0; j<vin; j++){
    InputType xup(in_base);
    doHost(xup, { xup_v.data()[j] += delta; });

    OutputType vup = model.value(xup);
    FloatType new_cost = testCost(c, vup);

    FloatType der = (new_cost - cost_base)/delta;
    doHost(inderiv_got, {
	FloatType der_got = inderiv_got_v.data()[j];
	std::cout << "Cost deriv wrt input linear idx " << j << " got " << der_got << " expect " << der << std::endl;
	assert(abs_near(der, der_got, 1e-4)); 
      });      
  }
};


//Same as the above but uses a wrapper object to linearize all inputs and outputs to vectors, allowing more complex usage patterns
/**
 * @brief Tests the derivative calculations of a component wrapper.
 *
 * This function checks the correctness of the derivative calculations 
 * of a component wrapper by comparing numerical approximations with 
 * analytical results. It tests both parameter derivatives and layer 
 * derivatives.
 *
 * @param cpt The component wrapper to be tested.
 * @param delta The step size used for numerical differentiation.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename ComponentWrapper>
void testComponentDeriv(ComponentWrapper &cpt, typename ComponentWrapper::FloatType delta = typename ComponentWrapper::FloatType(1e-4)){
  typedef typename ComponentWrapper::FloatType FloatType;
  
  std::mt19937 rng(1987);

  //Check basic functionality
  int nparam = cpt.nparams(); //assumed correct
  Vector<FloatType> base_params(nparam);
  random(base_params, rng);

  cpt.update(0, base_params);
  if(nparam > 0){
    Vector<FloatType> testp(nparam);
    cpt.getParams(testp,0);
    assert(equal(testp,base_params,true));
  }
  if(nparam > 0){
    Vector<FloatType> shifts(nparam);
    random(shifts, rng);
    cpt.step(0,shifts,0.33);
    
    Vector<FloatType> pexpect = base_params - 0.33*shifts;
    Vector<FloatType> pgot(nparam);
    cpt.getParams(pgot,0);
    assert(abs_near(pexpect, pgot, 1e-4, true));
    
    cpt.update(0, base_params);
  }

  //Check derivatives  
  size_t vout = cpt.outputLinearSize();
  size_t vin = cpt.inputLinearSize();
  
  //let  cost = \sum_i c_i * out_i
  //above_deriv = dcost/dout_i = c_i
  Vector<FloatType> c(vout);
  random(c,rng);

  Vector<FloatType> in_base(vin);
  random(in_base, rng);

  Vector<FloatType> val_base = cpt.value(in_base);
  Vector<FloatType> pderiv_got(nparam,0.);

  Vector<FloatType> inderiv_got;
  cpt.deriv(pderiv_got,0, Vector<FloatType>(c), inderiv_got);
 
  //Test parameter derivs
  //param_deriv = \sum_i dcost/dout_i  dout_i/dparam_j  =  \sum_i c_i dout_i/dparam_j
  FloatType cost_base = testCost(c, val_base);
  
  for(int j=0;j<nparam;j++){
    Vector<FloatType> pup(base_params);
    doHost(pup, { pup_v(j) += delta; });
    cpt.update(0,pup);
    
    Vector<FloatType> vup = cpt.value(in_base);
    FloatType new_cost = testCost(c, vup);

    FloatType der = (new_cost - cost_base)/delta;
    doHost(pderiv_got, {
	std::cout << "Cost deriv wrt param " << j << " got " << pderiv_got_v(j) << " expect " << der << std::endl;
	assert(abs_near(der, pderiv_got_v(j), 1e-4)); 
      });
  }

  //Test layer deriv
  //layer_deriv_j = \sum_i dcost/dout_i dout_i/din_j
  cpt.update(0,base_params);

  for(size_t j=0; j<vin; j++){
    Vector<FloatType> xup(in_base);
    doHost(xup, { xup_v.data()[j] += delta; });

    Vector<FloatType> vup = cpt.value(xup);
    FloatType new_cost = testCost(c, vup);

    FloatType der = (new_cost - cost_base)/delta;
    doHost(inderiv_got, {
	FloatType der_got = inderiv_got_v.data()[j];
	std::cout << "Cost deriv wrt input linear idx " << j << "=" << cpt.inCoord(j) << " got " << der_got << " expect " << der << std::endl;
	assert(abs_near(der, der_got, 1e-4)); 
      });      
  }
};


//Reference implementation of softmax
/**
 * @brief Computes the softmax vector of a given input vector.
 *
 * This function calculates the softmax values for each element in the input vector,
 * which can be used for various applications such as probability distributions.
 *
 * @param v Input vector of floating point numbers.
 * @param beta Temperature parameter that controls the softness of the output probabilities.
 * @return A new vector containing the softmax values corresponding to the input elements.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType>
std::vector<FloatType> softMaxVector(const std::vector<FloatType> &v, FloatType beta = 1.0){
  int np=v.size();
  FloatType max = v[0];
  for(int i=1;i<np;i++)
    max = std::max(max, v[i]);

  FloatType norm = exp(beta*(v[0]-max));
  for(int i=1;i<np;i++)
    norm += exp(beta*(v[i]-max));

  std::vector<FloatType> out(np);
  for(int i=0;i<np;i++)
    out[i] = exp(beta*(v[i]-max)) / norm;

  return out;
}
