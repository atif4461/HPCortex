#pragma once
#include "LayerCommon.hpp"

//The input layer
//This is always the lowest layer in the model
template<typename _FloatType, typename _InputType = Matrix<_FloatType> >
class InputLayer{  
public:
  typedef _FloatType FloatType;
  typedef _InputType InputType;
  typedef LeafTag tag;
  
  inline InputLayer(){}
  inline InputLayer(InputLayer &&r) = default;
  inline InputLayer(const InputLayer &r) = delete;

/**
 * @brief Returns the input value without modification.
 *
 * @param x The input value to be returned.
 * @return A constant reference to the input value.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline const InputType &value(const InputType &x){
    //std::cout << "InputLayer with tensor of dim " << x.dimension() << " and sizes " << x.sizeArrayString() << std::endl;
    
    //Simply reflect the passed-down input value back up to commence forwards propagation
    return x;
  }

  //input_above_deriv_return is the derivative of the cost with respect to the inputs
  inline void deriv(Vector<FloatType> &cost_deriv, int off, InputType &&above_deriv, InputType* input_above_deriv_return = nullptr) const{
    //We don't have to do anything for backpropagation as this is the last layer
    if(input_above_deriv_return) *input_above_deriv_return = std::move(above_deriv); //copy back the input derivative if desired
  }
  
  inline void update(int off, const Vector<FloatType> &new_params){}

  inline void step(int off, const Vector<FloatType> &derivs, FloatType eps){}
  
  inline int nparams() const{ return 0; }

  inline void getParams(Vector<FloatType> &into, int off){}

  //For pipelining
  inline void resizeInputBuffer(size_t to){}
};

template<typename FloatType, typename InputType = Matrix<FloatType> >
inline InputLayer<FloatType,InputType> input_layer(){ return InputLayer<FloatType,InputType>(); }
