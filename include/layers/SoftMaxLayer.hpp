#pragma once
#include "LayerCommon.hpp"
#include <components/SoftMaxComponent.hpp>

//Inputs are assumed to be matrices of size k * b   where b is the batch size. The softmax normalization is performed over k for fixed b
/**
 * @brief Represents a softmax layer in a neural network.
 *
 * This class encapsulates the functionality of a softmax layer,
 * including computation of output values, derivatives, and parameter updates.
  
 * @brief Represents a component of the softmax layer.
 *
 * This class provides methods for computing the softmax activation function
 * and its derivative.
 
 * @brief Stores parameters and intermediate results of the softmax layer.
 *
 * This class manages the storage and retrieval of parameters and intermediate
 * results used during the forward and backward passes of the softmax layer.
  
 * @brief Computes the output value of the softmax layer for a given input.
 *
 * @param x The input to the softmax layer.
 * @return The computed output value.
 
Matrix<FloatType> value(const InputType &x);
 * @brief Computes the derivative of the loss with respect to the inputs of the softmax layer.
 *
 * @param cost_deriv The derivative of the loss with respect to the outputs of the softmax layer.
 * @param off Offset index.
 * @param above_deriv Derivative of the loss with respect to the outputs of the layer above.
 * @param input_above_deriv_return Optional pointer to store the derivative of the loss with respect to the inputs of the layer above.
 
void deriv(Vector<FloatType> &cost_deriv, int off, Matrix<FloatType> &&above_deriv, InputType* input_above_deriv_return = nullptr) const;
 * @brief Updates the parameters of the softmax layer.
 *
 * @param off Offset index.
 * @param new_params New parameter values.
 
inline void update(int off, const Vector<FloatType> &new_params);
 * @brief Performs a gradient descent step to update the parameters of the softmax layer.
 *
 * @param off Offset index.
 * @param derivs Gradient of the loss with respect to the parameters.
 * @param eps Learning rate.
 
inline void step(int off, const Vector<FloatType> &derivs, FloatType eps);
 * @brief Returns the number of parameters in the softmax layer.
 *
 * @return Number of parameters.
 
inline int nparams() const;
 * @brief Retrieves the current parameter values of the softmax layer.
 *
 * @param into Output vector to store the parameter values.
 * @param off Offset index.
 
inline void getParams(Vector<FloatType> &into, int off);
 * @brief Resizes the input buffer for pipelining.
 *
 * @param to New size of the input buffer.
 
inline void resizeInputBuffer(size_t to);* This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename _FloatType, typename _InputType, typename Store >
class SoftMaxLayer{  
public:
  typedef _FloatType FloatType;
  typedef _InputType InputType;
  typedef LeafTag tag;
private:
  Store leaf;
  SoftMaxComponent<FloatType,2> cpt;
public:
  
  inline SoftMaxLayer(Store &&leaf, FloatType beta = 1.0): leaf(std::move(leaf)), cpt(0,beta){}
  inline SoftMaxLayer(SoftMaxLayer &&r) = default;
  inline SoftMaxLayer(const SoftMaxLayer &r) = delete;

  Matrix<FloatType> value(const InputType &x);
  
  void deriv(Vector<FloatType> &cost_deriv, int off, Matrix<FloatType> &&above_deriv, InputType* input_above_deriv_return = nullptr) const;
  
  inline void update(int off, const Vector<FloatType> &new_params){ leaf.v.update(off, new_params); }

  inline void step(int off, const Vector<FloatType> &derivs, FloatType eps){ leaf.v.step(off,derivs,eps); }
  
  inline int nparams() const{ return leaf.v.nparams(); }

  inline void getParams(Vector<FloatType> &into, int off){ leaf.v.getParams(into,off); }

  //For pipelining
  inline void resizeInputBuffer(size_t to){
    cpt.resizeInputBuffer(to);
    leaf.v.resizeInputBuffer(to);
  }
};

template<typename U, typename std::enable_if<ISLEAF(U), int>::type = 0>
auto softmax_layer(U &&u, FLOATTYPE(U) beta)->SoftMaxLayer<FLOATTYPE(U),INPUTTYPE(U),DDST(u)>{
  return SoftMaxLayer<FLOATTYPE(U),INPUTTYPE(U),DDST(u)>(std::forward<U>(u), beta);
}

#include "implementation/SoftMaxLayer.tcc"

