/**
 * @brief Computes the output value of the skip connection layer.
 *
 * This method takes an input of type InputType, passes it through the leaf below and internal layers,
 * and returns the sum of their outputs as a Matrix of FloatType values.
 *
 * @param x The input to the skip connection layer.
 * @return A Matrix of FloatType values representing the output of the skip connection layer.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, typename InputType, typename ChainInternal, typename ChainBelow>
Matrix<FloatType> SkipConnection<FloatType,InputType,ChainInternal,ChainBelow>::value(const InputType &x){
  Matrix<FloatType> in = leaf_below.v.value(x);
  Matrix<FloatType> out = in + leaf_internal.v.value(in);
  
  in_buf.push(std::move(in));
  in_size = in.size(0);
  batch_size = in.size(1);
  
  return out;
}

/**
 * @brief Computes derivatives of the cost function with respect to the model parameters and inputs.
 *
 * This function calculates the derivative of the cost function with respect to both the model parameters and inputs,
 * by propagating the derivatives through the layers of the network.
 *
 * @param[out] cost_deriv Vector containing the derivatives of the cost function with respect to the model parameters.
 * @param[in] off Offset index for parameter indexing.
 * @param[in] _above_deriv Derivative of the cost function with respect to the output of the previous layer.
 * @param[out] input_above_deriv_return Pointer to store the derivative of the cost function with respect to the input of the current layer.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, typename InputType, typename ChainInternal, typename ChainBelow>
void SkipConnection<FloatType,InputType,ChainInternal,ChainBelow>::deriv(Vector<FloatType> &cost_deriv, int off, Matrix<FloatType> &&_above_deriv, InputType* input_above_deriv_return) const{
  assert(_above_deriv.size(0) == in_size);
  assert(_above_deriv.size(1) == batch_size);
  int p=off;
  Matrix<FloatType> layer_deriv;
  {
    Matrix<FloatType> above_deriv(std::move(_above_deriv)); //inside the braces above ensures this object is freed before the next layer is called
      
    //until the pipeline is "primed", the ring buffers will pop uninitialized values. We could in principle skip doing any computation until then
    //but for now we just initialize with zero values (TODO: revisit)
    Matrix<FloatType> in = in_buf.isFilled() ? in_buf.pop(): Matrix<FloatType>(in_size,batch_size,0.);
    assert(in.size(0) == in_size);
    assert(in.size(1) == batch_size);
      
    //f_i(x) = g_i(x) + x_i

    //deriv wrt inputs for backprop
    //df_i/dx_j = dg_i/dx_j + delta_ij
      
    //dcost / dx_j = \sum_i dcost/df_i df_i/dx_j
    //             = \sum_i dcost/df_i dg_i/dx_j  + \sum_i dcost/df_i delta_ij
    //             = \sum_i dcost/df_i dg_i/dx_j  + \sum_i dcost/df_j
      
    //deriv wrt params for filling cost_deriv
    //df_i/dparam_p = dg_i/dparam_p

    layer_deriv = above_deriv; //dcost/df_j
    Matrix<FloatType> leaf_internal_deriv; //\sum_i dcost/df_i dg_i/dx_j
    leaf_internal.v.deriv(cost_deriv, p, std::move(above_deriv), &leaf_internal_deriv);

    layer_deriv += leaf_internal_deriv;

    p += leaf_internal.v.nparams();  
  }//close views and free temporaries before calling layer below
    
  leaf_below.v.deriv(cost_deriv, p, std::move(layer_deriv), input_above_deriv_return);
}
/**
 * Updates the connection by applying new parameters starting at offset off.
 * @param off The offset at which to start updating parameters.
 * @param new_params The vector of new parameters to apply.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, typename InputType, typename ChainInternal, typename ChainBelow>
void SkipConnection<FloatType,InputType,ChainInternal,ChainBelow>::update(int off, const Vector<FloatType> &new_params){
  int p=off;
  leaf_internal.v.update(p, new_params);
  p += leaf_internal.v.nparams();
  leaf_below.v.update(p, new_params);
}

/**
 * @brief Performs a step operation on the connection with offset and derivatives.
 * @param off Offset value for the step operation.
 * @param derivs Derivatives used in the step calculation.
 * @param eps Epsilon value used in the step calculation.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, typename InputType, typename ChainInternal, typename ChainBelow>
void SkipConnection<FloatType,InputType,ChainInternal,ChainBelow>::step(int off, const Vector<FloatType> &derivs, FloatType eps){
  int p=off;
  leaf_internal.v.step(p, derivs, eps);
  p += leaf_internal.v.nparams();
  leaf_below.v.step(p, derivs, eps);
}

//off measured from *end*, return new off
/**
 * Retrieves parameters from internal and below chain components.
 *
 * @param[in,out] into Vector to store retrieved parameters
 * @param[in] off Offset index for parameter retrieval
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
template<typename FloatType, typename InputType, typename ChainInternal, typename ChainBelow>
void SkipConnection<FloatType,InputType,ChainInternal,ChainBelow>::getParams(Vector<FloatType> &into, int off){
  int p = off;
  leaf_internal.v.getParams(into, p);
  p += leaf_internal.v.nparams();
  leaf_below.v.getParams(into,p);
}
