#include<iostream>
#include<cmath>
#include<vector>
#include <random>
#include <algorithm>
#include <cassert>
#include <array>
#include <memory>
#include <mpi.h>
#include "RingBuffer.h"

//ML interface with batching
struct LeafTag{};

template<size_t dim>
/**
 * @brief Calculates the total number of elements in a tensor.
 *
 * @param dims Dimension sizes of the tensor.
 * @return Total number of elements in the tensor.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
inline size_t tensorSize(const std::array<int,dim> &dims){
  size_t out=1;
  for(int d=0;d<dim;d++) out *= dims[d];
  return out;
}
template<size_t Dim>
/**
 * @brief Computes the offset of a point in a multidimensional array
 * @param[in] coord coordinates of the point
 * @param[in] dims dimensions of the array
 * @return the computed offset
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
inline size_t compute_off(int const* coord, int const* dims){
  size_t out = *coord++; ++dims;
  for(int i=1;i<Dim;i++) out = out * (*dims++) + (*coord++);
  return out;
}
template<size_t Dim>
inline size_t compute_off(const std::array<int,Dim> &coord, const std::array<int,Dim> &dims){
  return compute_off<Dim>(coord.data(),dims.data());
}

template<int Dim>
struct Tensor{
  std::vector<double> vals;
  int _size[Dim];
    
public:
  typedef std::array<int,Dim> Dims;
  typedef std::array<int,Dim> Coord;
  
  constexpr int dimension(){ return Dim; }
  Tensor(): _size{0}{}
  Tensor(const Dims &dims, double init): vals(tensorSize(dims),init){ memcpy(_size,dims.data(),Dim*sizeof(int));  }
/**
 * @brief Constructs a tensor with specified dimensions and initial values.
 * @param dims Dimensions of the tensor.
 * @param init_vals Initial values of the tensor elements.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  Tensor(const Dims &dims, const std::vector<double> &init_vals): vals(init_vals){
    memcpy(_size,dims.data(),Dim*sizeof(int));
    assert(tensorSize(dims) == init_vals.size());
  }  
  inline double & operator()(const Coord &coord){ return vals[compute_off<Dim>(coord.data(), size)]; }
  inline double operator()(const Coord &coord) const{ return vals[compute_off<Dim>(coord.data(), size)]; }

  inline int size(int i) const{ return _size[i]; }

};

struct Vector{
  std::vector<double> vals;
public:
  Vector(){}
  Vector(int size1): vals(size1){}
  Vector(int size1, double init): vals(size1, init){}
  Vector(const std::vector<double> &init_vals): vals(init_vals){}    
  
  inline double & operator()(const int i){ return vals[i]; }
  inline double operator()(const int i) const{ return vals[i]; }

  inline int size(int i) const{ return vals.size(); }

  double const* data() const{ return vals.data(); }
  double* data(){ return vals.data(); }
  size_t data_len() const{ return vals.size(); }
};

/**
 * @brief Output vector contents to output stream
 * @param os reference to output stream
 * @param v constant reference to vector object
 * @return reference to output stream
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
std::ostream & operator<<(std::ostream &os, const Vector &v){
  if(v.size(0)==0){ os << "()"; return os; }    
  os << "(" << v(0);
  for(int i=1;i<v.size(0);i++) os << ", " << v(i);
  os << ")";
  return os;  
}

struct Matrix{
  std::vector<double> vals;
  int size0;
  int size1;
public:
  Matrix(): size0(0),size1(0){}
  Matrix(int size0, int size1): size0(size0), size1(size1), vals(size0*size1){}  
  Matrix(int size0, int size1, double init): size0(size0), size1(size1), vals(size0*size1,init){}
  Matrix(int size0, int size1, const std::vector<double> &init_vals): size0(size0), size1(size1), vals(init_vals){}    
  
  inline double & operator()(const int i, const int j){ return vals[j+size1*i]; }
  inline double operator()(const int i, const int j) const{ return vals[j+size1*i]; }

  inline int size(int i) const{ return i==0 ? size0 : size1; }

/**
 * @brief Sets the values of a column in the matrix.
 * @param col The column index to be modified.
 * @param data A vector containing the new column values.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void pokeColumn(int col, const Vector &data){
    assert(data.size(0) == size0);
    for(int i=0;i<size0;i++)
      this->operator()(i,col) = data(i);
  }
/**
 * @brief Returns a column vector at specified column index.
 * @param col Column index.
 * @return Vector containing elements of the specified column.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  Vector peekColumn(int col) const{
    Vector out(size0);
    for(int i=0;i<size0;i++) out(i)=this->operator()(i,col);
    return out;
  }
  double const* data() const{ return vals.data(); }
  double* data(){ return vals.data(); }
  size_t data_len() const{ return vals.size(); }
   
};

/**
 * @brief Outputs a matrix to an output stream.
 *
 * Prints the elements of a matrix to the specified output stream,
 * with rows separated by newlines and columns separated by commas.
 *
 * @param os The output stream to print to.
 * @param v The matrix to be printed.
 * @return A reference to the output stream.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
std::ostream & operator<<(std::ostream &os, const Matrix &v){
  if(v.size(0)==0 || v.size(1) == 0){ os << "||"; return os; }
  for(int r=0;r<v.size(0);r++){
    os << "|" << v(r,0);
    for(int i=1;i<v.size(1);i++) os << ", " << v(r,i);
    os << "|";
    if(r != v.size(0)-1) os << std::endl;
  }
  return os;  
}


/**
 * @brief Multiplies a matrix by a vector.
 * @param A The input matrix.
 * @param x The input vector.
 * @return The resulting vector from the multiplication.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
Vector operator*(const Matrix &A, const Vector &x){
  Vector out(A.size(0), 0.);
  for(int i=0;i<A.size(0);i++)
    for(int j=0;j<A.size(1);j++)
      out(i) += A(i,j) * x(j);
  return out;
}
/**
 * @brief Adds two vectors element-wise.
 * @param a The first vector.
 * @param b The second vector.
 * @return A new vector containing the sum of corresponding elements from a and b.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
Vector operator+(const Vector &a, const Vector &b){
  Vector out(a.size(0));
  for(int i=0;i<a.size(0);i++)
    out(i) = a(i) + b(i);
  return out;
}
/**
 * @brief Subtracts two vectors element-wise.
 * @param a The first vector.
 * @param b The second vector.
 * @return A new vector containing the difference between corresponding elements of a and b.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
Vector operator-(const Vector &a, const Vector &b){
  Vector out(a.size(0));
  for(int i=0;i<a.size(0);i++)
    out(i) = a(i) - b(i);
  return out;
}
/**
 * @brief Scales a vector by a scalar value.
 * @param eps scaling factor
 * @param b input vector
 * @return scaled vector
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
Vector operator*(double eps, const Vector &b){
  Vector out(b.size(0));
  for(int i=0;i<b.size(0);i++)
    out(i) = eps * b(i);
}


class InputLayer{
public:
  typedef LeafTag tag;
  
  inline InputLayer(){}
  inline InputLayer(InputLayer &&r) = default;
  inline InputLayer(const InputLayer &r) = delete;
  
  inline const Matrix &value(const Matrix &x){
    return x;
  }

  inline void deriv(Vector &cost_deriv, int off, const Matrix &above_deriv, Matrix* input_above_deriv_copyback = nullptr) const{
    if(input_above_deriv_copyback) *input_above_deriv_copyback = above_deriv;
  }
  
  inline void update(int off, const Vector &new_params){}

  inline void step(int off, const Vector &derivs, double eps){}
  
  inline int nparams() const{ return 0; }

  inline void getParams(Vector &into, int off){}

  //For pipelining
  inline void resizeInputBuffer(size_t to){}
};

inline InputLayer input_layer(){ return InputLayer(); }

struct StorageTag{};

template<typename T>
struct LeafStore{
  T v;
  typedef StorageTag tag;
  typedef T type;
  
  LeafStore(T && v): v(std::move(v)){
    //std::cout << "STORE" << std::endl;
  }
  LeafStore(const LeafStore &r) = delete;
  LeafStore(LeafStore &&r): v(std::move(r.v)){}
  
};
template<typename T>
struct LeafRef{
  T &v;
  typedef StorageTag tag;
  typedef T type;
  
  LeafRef(T &v): v(v){
    //std::cout << "REF" << std::endl;
  }
  LeafRef(const LeafRef &r) = delete;
  LeafRef(LeafRef &&r): v(r.v){}

};



template<typename T>
struct deduceStorage{};
template<typename T>
struct deduceStorage<T&>{
  typedef LeafRef<T> type;
};

template<typename T>
struct deduceStorage<T&&>{
  typedef LeafStore<T> type;
};

#define DDST(a) typename deduceStorage<decltype(a)>::type

#define ISLEAF(a) std::is_same<typename std::decay<a>::type::tag,LeafTag>::value

#define ISSTORAGE(a) std::is_same<typename std::decay<a>::type::tag,StorageTag>::value

class ReLU{
public: 
/**
 * @brief Applies the ReLU activation function element-wise to the input matrix.
 *
 * @param x Input matrix to apply the ReLU function to.
 * @return Resulting matrix after applying the ReLU function.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline Matrix operator()(const Matrix &x) const{
    int dim = x.size(0);
    int batch_size = x.size(1);
    Matrix out(dim,batch_size,1.0);
    //f(x)_i = max(x_i, 0)
    for(int i=0;i<dim;i++)
      for(int b=0;b<batch_size;b++)
	if(x(i,b) <= 0.) out(i,b) = 0.;
    return out;
  }
};

class noActivation{
public:
  inline Matrix operator()(const Matrix &x) const{
    return Matrix(x.size(0),x.size(1),1.0);
  }
};

  


template<typename Store, typename ActivationFunc>
class DNNlayer{
  Matrix weights;
  Vector bias;  
  Store leaf;
  int size0;
  int size1;

  ActivationFunc activation_func;

  //Storage from last call to "value"
  //Buffer size > 1 depending on rank if doing pipelining
  RingBuffer<Matrix> leaf_buf;
  RingBuffer<Matrix> activation_buf;
  size_t calls;

  int rank;
  bool pipeline_mode;
public:
  typedef LeafTag tag;
  
/**
 * @brief Constructor for DNN layer initialization
 * @param leaf Store object containing neural network data
 * @param weights Matrix representing synaptic weights
 * @param bias Vector representing bias values
 * @param activation_func Activation function used in the layer
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  DNNlayer(Store &&leaf, const Matrix &weights,const Vector &bias, const ActivationFunc &activation_func):
    leaf(std::move(leaf)), weights(weights), bias(bias),
    size0(weights.size(0)), size1(weights.size(1)),
    activation_func(activation_func), leaf_buf(1), calls(0), pipeline_mode(false)
  {
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  }
  DNNlayer(const DNNlayer &r) = delete;
  DNNlayer(DNNlayer &&r) = default;
  
  //Forward pass
  Matrix value(const Matrix &x){
    ++calls;
    
    Matrix in = leaf.v.value(x);
    int batch_size = x.size(1);   
    assert(in.size(0) == size1);
    assert(in.size(1) == batch_size);

    leaf_buf.push(in);
    //if(pipeline_mode) std::cout << "RANK " << rank << " " << this << " CALL " << calls << " INPUT " << x << " VALUE BUFFERED INPUT " << in << std::endl;
    //else std::cout << "RANK " << rank << " " << this << " UNPIPELINED CALL " << calls << " INPUT " << x << " VALUE BUFFERED INPUT " << in << std::endl;
    
    Matrix out(size0,batch_size,0.0);

    for(int i=0;i<size0;i++){
      for(int b=0;b<batch_size;b++){
	out(i,b) = bias(i);
	for(int j=0;j<size1;j++)
	  out(i,b) += weights(i,j)* in(j,b);
      }
    }
	
    Matrix activation = activation_func(out);
    assert(activation.size(0) == size0);
    assert(activation.size(1) == batch_size);
    
    for(int i=0;i<size0;i++)
      for(int b=0;b<batch_size;b++)
	out(i,b) *= activation(i,b);    

    activation_buf.push(activation);
    
    return out;
  }
 
/**
 * @brief Computes the derivative of the cost function with respect to the model parameters.
 *
 * This function calculates the partial derivatives of the cost function with respect to the model weights and biases,
 * using the chain rule and the previously computed derivatives of the cost function with respect to the output of the current layer.
 *
 * @param[out] cost_deriv The vector containing the partial derivatives of the cost function with respect to the model parameters.
 * @param[in] off Offset index for the parameter derivatives in the cost_deriv vector.
 * @param[in] above_deriv The matrix containing the derivatives of the cost function with respect to the output of the current layer.
 * @param[out] input_above_deriv_copyback Optional pointer to store a copy of the input above_deriv matrix.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void deriv(Vector &cost_deriv, int off, const Matrix &above_deriv, Matrix* input_above_deriv_copyback = nullptr) const{
    assert(above_deriv.size(0) == size0);
    Matrix in = leaf_buf.pop();
    Matrix activation = activation_buf.pop();
    int batch_size = in.size(1);

    //if(pipeline_mode) std::cout << "RANK " << rank << " " << this << " CALL " << calls << " DERIV USING BUFFERED INPUT " << in << " ABOVE_DERIV " << above_deriv << " WITH INPUT COST DERIV " << cost_deriv;
    //else std::cout << "RANK " << rank << " " << this << " UNPIPELINED CALL " << calls << " DERIV USING BUFFERED INPUT " << in << " ABOVE_DERIV " << above_deriv << " WITH INPUT COST DERIV " << cost_deriv;
    
    //for reverse differentiation, we pass down the derivatives with respect to our inputs
    //f(x)_i = act_i b_i + \sum_j act_i w_ij x_j
    //dcost / dx_j = \sum_i dcost/df_i df_i/dx_j
    //df_i/dx_j = act_i w_ij
    Matrix layer_deriv(size1,batch_size,0.);
    for(int j=0;j<size1;j++)
      for(int i=0;i<size0;i++)
	for(int b=0;b<batch_size;b++)
	  layer_deriv(j,b) += above_deriv(i,b) * activation(i,b) * weights(i,j);

    //Now we finish up the derivs wrt our parameters
    //df(x)_i / d w_jk = delta_ij act_j x_k
    //df(x)_i / d b_j = delta_ij act_j
    //dcost / dw_jk = \sum_i dcost/df_i df_i/dw_jk = dcost/df_j * act_j * x_k
    //dcost / db_j = \sum_i dcost/df_i df_i/db_j = dcost/df_j * act_j
    int p=off;
    for(int j=0;j<size0;j++)
      for(int k=0;k<size1;k++){
	for(int b=0;b<batch_size;b++)
	  cost_deriv(p) += above_deriv(j,b) * activation(j,b) * in(k,b); //batch reduction! (assume zero-initialized)
	++p;
      }
	
    for(int j=0;j<size0;j++){
      for(int b=0;b<batch_size;b++)
	cost_deriv(p) += above_deriv(j,b) * activation(j,b);
      ++p;
    }

    //std::cout << " AND RESULT " << cost_deriv << std::endl;
    
    leaf.v.deriv(cost_deriv, p, layer_deriv, input_above_deriv_copyback);
  }

/**
 * @brief Updates model parameters at specified offset with new values.
 * @param off Offset where updates start.
 * @param new_params New parameter values to be applied.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void update(int off, const Vector &new_params){
    int p=off;
    for(int i=0;i<size0;i++)
      for(int j=0;j<size1;j++)
	weights(i,j) = new_params(p++);
    for(int i=0;i<size0;i++)
      bias(i) = new_params(p++);
    leaf.v.update(p, new_params);
  }
/**
 * @brief Updates model parameters based on computed derivatives and learning rate.
 * @param off Offset index for accessing derivative values.
 * @param derivs Array of computed derivatives.
 * @param eps Learning rate for updating model parameters.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void step(int off, const Vector &derivs, double eps){
    int p=off;
    for(int i=0;i<size0;i++)
      for(int j=0;j<size1;j++){
	//std::cout << "Weights " << i << " " << j << " " << weights(i,j) << " -= " << derivs(p) << "*" << eps;
	weights(i,j) -= derivs(p++) * eps;
	//std::cout << " = " <<  weights(i,j) << std::endl;
      }
    for(int i=0;i<size0;i++){
      //std::cout << "Bias " << i << " " << bias(i) << " -= " << derivs(p) << "*" << eps;
      bias(i) -= derivs(p++) * eps;
      //std::cout << " = " << bias(i) << std::endl;
    }
    leaf.v.step(p, derivs, eps);
  }

  //accumulated #params for layers here and below
  inline int nparams() const{ return size0*size1 + size0 + leaf.v.nparams(); }

  //off measured from *end*, return new off
  void getParams(Vector &into, int off){
    int p = off;
    for(int i=0;i<size0;i++)
      for(int j=0;j<size1;j++)
	into(p++) = weights(i,j);
    for(int i=0;i<size0;i++)
      into(p++) = bias(i);
    leaf.v.getParams(into, p);
  }

  //For pipelining
  inline void resizeInputBuffer(size_t to){
    //std::cout << "RANK " << rank << " " << this << " RESIZING RING BUFFERS TO " << to << std::endl;
    pipeline_mode = true;
    leaf_buf.resize(to);
    activation_buf.resize(to);
    leaf.v.resizeInputBuffer(to);
  }

};

template<typename U, typename ActivationFunc, typename std::enable_if<ISLEAF(U), int>::type = 0>
auto dnn_layer(U &&u, const Matrix &weights,const Vector &bias, const ActivationFunc &activation)->DNNlayer<DDST(u),ActivationFunc>{
  return DNNlayer<DDST(u),ActivationFunc>(std::forward<U>(u), weights, bias, activation);
}
template<typename U, typename std::enable_if<ISLEAF(U), int>::type = 0>
auto dnn_layer(U &&u, const Matrix &weights,const Vector &bias)->DNNlayer<DDST(u),noActivation>{
  return DNNlayer<DDST(u),noActivation>(std::forward<U>(u), weights, bias, noActivation());
}

class MSEcostFunc{
public:
/**
 * @brief Calculates the mean squared error between two matrices.
 * @param y The target matrix.
 * @param ypred The predicted matrix.
 * @return The mean squared error value.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline static double loss(const Matrix &y, const Matrix &ypred){
    int dim = y.size(0);
    int batch_size = y.size(1);
    
    double out = 0.;
    for(int i=0;i<dim;i++)
      for(int b=0;b<batch_size;b++)
	out += pow(ypred(i,b) - y(i,b),2);
    out /= (dim * batch_size);
    return out;
  }
/**
 * @brief Computes the derivative of the layer output with respect to its input.
 *
 * This function calculates the derivative of the difference between predicted and actual outputs,
 * which is used for backpropagation in neural networks.
 *
 * @param y The actual output matrix.
 * @param ypred The predicted output matrix.
 * @return A matrix containing the derivatives of the layer output with respect to its input.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline static Matrix layer_deriv(const Matrix &y, const Matrix &ypred){
    //for reverse differentiation, we pass down the derivatives with respect to our inputs
    //dout / dparam(i) = \sum_j 2*(ypred(j) - y(j)) * dypred(j)/dparam(i)

    //dout / dypred(i) = 2*(ypred(i) - y(i)) /dim
    int dim = y.size(0);
    int batch_size = y.size(1);
    
    Matrix layer_deriv(dim,batch_size);
    for(int i=0;i<dim;i++)
      for(int b=0;b<batch_size;b++)
	layer_deriv(i,b) = 2*(ypred(i,b) - y(i,b)) / (dim*batch_size);
    return layer_deriv;
  }
};


template<typename Store, typename CostFunc>
class CostFuncWrapper{
  Store leaf;
  Matrix ypred; //dim * batch_size
  Matrix yval;
  CostFunc cost;
  int nparam;
public:
  CostFuncWrapper(Store &&leaf, const CostFunc &cost = CostFunc()): cost(cost), leaf(std::move(leaf)), nparam(leaf.v.nparams()){}
  
/**
 * @brief Calculates the loss between predicted and actual values.
 *
 * @param x Input matrix used for prediction.
 * @param y Actual output matrix.
 * @return Calculated loss value.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  double loss(const Matrix &x, const Matrix &y){
    ypred = leaf.v.value(x);
    int dim = y.size(0);
    int batch_size = y.size(1);
    assert(ypred.size(0) == dim);
    assert(ypred.size(1) == batch_size);
    
    yval = y;
    return cost.loss(y,ypred);
  }
/**
 * @brief Computes the derivative of the cost function with respect to model parameters.
 *
 * @return A vector containing the derivatives of the cost function.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  Vector deriv() const{
    Matrix layer_deriv = cost.layer_deriv(yval, ypred);

    Vector cost_deriv(nparam,0.);    //zero initialize
    leaf.v.deriv(cost_deriv, 0, layer_deriv);
    return cost_deriv;
  }

  Matrix predict(const Matrix &x){
    return leaf.v.value(x);
  }

  void update(const Vector &new_params){
    leaf.v.update(0, new_params);
  }
  void step(const Vector &derivs, double eps){
    leaf.v.step(0,derivs,eps);
  }
  int nparams(){ return nparam; }

/**
 * @brief Retrieves model parameters.
 *
 * @return Vector containing model parameters.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  Vector getParams(){
    Vector out(nparams());
    leaf.v.getParams(out,0);
    return out;
  }
};
  
    
template<typename U, typename std::enable_if<ISLEAF(U), int>::type = 0>
auto mse_cost(U &&u)->CostFuncWrapper<DDST(u), MSEcostFunc>{
  return CostFuncWrapper<DDST(u), MSEcostFunc>(std::forward<U>(u));
}


template<typename PipelineBlockType, typename CostFunc>
class PipelineCostFuncWrapper{
  PipelineBlockType &block;

  RingBuffer<Matrix> yval_buf_v;//buffered yvalues for calls to "loss"
  //RingBuffer<Matrix> yval_buf_d;//buffered yvalues for calls to "deriv" (these have a longer lag time)
  //RingBuffer<Matrix> ypred_buf_d;//buffered ypred for calls to "deriv" 
  size_t calls;

  Matrix ypred; //dim * batch_size
  Matrix yval; //yval associated with ypred
  
  CostFunc cost;
  int nparam;
  int value_lag;
  int deriv_lag;

  int rank;
public:
/**
 * @brief Constructor for PipelineCostFuncWrapper class
 * @param block Reference to PipelineBlockType object
 * @param cost Constant reference to CostFunc object (default constructed if not provided)
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  PipelineCostFuncWrapper(PipelineBlockType &block, const CostFunc &cost = CostFunc()): cost(cost), block(block), nparam(block.nparams()),
											value_lag(block.valueLag()), deriv_lag(block.derivLag()),
											yval_buf_v(block.valueLag()),
											calls(0){
    //yval_buf_off(0),
    //yval_buf_d(block.derivLag()-block.valueLag()+1),
    //ypred_buf_d(block.derivLag()-block.valueLag()+1),

    
    MPI_Comm_rank(MPI_COMM_WORLD,&rank);
  }


  
/**
 * @brief Calculates the loss between predicted and actual output values.
 *
 * This function takes two matrices as input, representing the predicted and actual outputs,
 * and returns the calculated loss. It utilizes a ring buffer to store previous actual output values.
 *
 * @param x The input matrix containing the predicted output values.
 * @param y The input matrix containing the actual output values.
 * @return The calculated loss between the predicted and actual output values.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  double loss(const Matrix &x, const Matrix &y){
    ++calls;
    int dim = y.size(0);
    int batch_size = y.size(1);
    
    //<i- (<0-, <1- etc): item i in 'prev_in' at start of iteration, perform action and send left in this iter
    //<i|                : take item i from input 'in', perform action and send left in this iter
    //iter    rank->
    //        0     1      2
    //0                   <0|
    //1            <0-    <1|
    //2      <0-   <1-    <2|
    //3      <1-   <2-    <3|
    //etc
    //value_lag = 3 = nrank

    yval_buf_v.push(y);
   
    ypred = block.value(x);
    assert(ypred.size(0) == dim);
    assert(ypred.size(1) == batch_size);
    
    if(calls < value_lag) return -1;
    else{ //yval not initialized until ring buffer is full
      yval = yval_buf_v.pop();
      assert(yval.size(0) == dim);
      assert(yval.size(1) == batch_size);
      
      return cost.loss(yval,ypred);
    }
  }
  
/**
 * @brief Computes the derivative of the cost with respect to model parameters.
 *
 * This function calculates the derivative of the cost function with respect to the model parameters,
 * taking into account the specified derivation lag.
 *
 * @return A vector containing the derivatives of the cost with respect to each model parameter.
 *         Returns a vector of -1 values if the number of calls is less than the derivation lag.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  Vector deriv() const{
    //3 ranks
    //Value:
    //iter    rank->
    //        0     1      2
    //0                   <0|
    //1            <0-    <1|
    //2      <0-   <1-    <2|
    //3      <1-   <2-    <3|
    //etc
    
    //value lag: 3

    //Deriv:
    //iter    rank->
    //        0     1      2
    //0       
    //1       
    //2      |0>   
    //3      |1>   -0>
    //4      |2>   -1>    -0>    
    //etc

    //deriv lag: 5

    //Notice rank 0 consumes y,ypred[i] on the same iteration it receives ypred[i], so we don't need a buffer
    if(calls >= value_lag){ //can't call before value_lag because yval uninitialized
      Matrix layer_deriv = cost.layer_deriv(yval, ypred);
      Vector cost_deriv(nparam,0.);    //zero initialize
      block.deriv(cost_deriv, layer_deriv);
      if(calls < deriv_lag) return Vector(nparam,-1.); //indicate that these derivs are invalid
      else return cost_deriv;
    }else return Vector(nparam,-1.); //indicate that these derivs are invalid
  }
  
  
};

template<typename BlockStore>
class PipelineBlock{
  BlockStore block; //this chain should terminate on an InputLayer. This represents the work done on this rank

  int rank; //current rank
  int next_rank; //rank of next comm layer, -1 indicates this is the last
  int prev_rank; //rank of previous comm layer, -1 indicates this is the first
  int pipeline_depth; //number of ranks in the pipeline
  bool is_first;
  bool is_last;
  
  int batch_size;
  int input_features; //features of data
  int this_features; //features of this layer
  int next_features; //features of output of layer to the right
  
  int nparam; //total #params
  int stage_off; //offset within parameter vector associated with this block
  
  Matrix prev_in; //input for forwards propagation

  //storage for backpropagation
  Matrix prev_above_deriv;
  Vector prev_cost_deriv_passright;
  
/**
 * @brief Sends a matrix to another process asynchronously.
 * @param mat The matrix to be sent.
 * @param to The rank of the destination process.
 * @return A request object representing the ongoing communication operation.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline static MPI_Request send(const Matrix &mat, int to){
    MPI_Request req;		
    MPI_Isend(mat.data(), mat.data_len(), MPI_DOUBLE, to, 0, MPI_COMM_WORLD, &req);
    return req;
  }
/**
 * @brief Receives a matrix from another process.
 *
 * @param[in,out] mat Matrix object to receive data into
 * @param[in] from Rank of the process sending the matrix
 * @return Request handle for the non-blocking receive operation
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline static MPI_Request recv(Matrix &mat, int from){
    MPI_Request req;		
    MPI_Irecv(mat.data(), mat.data_len(), MPI_DOUBLE, from, 0, MPI_COMM_WORLD, &req);
    return req;
  }
/**
 * @brief Sends a vector of doubles to another process asynchronously.
 * @param mat The vector of doubles to be sent.
 * @param to The rank of the destination process.
 * @return A request object representing the ongoing communication operation.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline static MPI_Request send(const Vector &mat, int to){
    MPI_Request req;		
    MPI_Isend(mat.data(), mat.data_len(), MPI_DOUBLE, to, 0, MPI_COMM_WORLD, &req);
    return req;
  }
/**
 * @brief Receives a vector of doubles from another process.
 *
 * @param[in] mat Vector to receive data into
 * @param[in] from Rank of the process sending the data
 * @return Request object representing the reception operation
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline static MPI_Request recv(Vector &mat, int from){
    MPI_Request req;		
    MPI_Irecv(mat.data(), mat.data_len(), MPI_DOUBLE, from, 0, MPI_COMM_WORLD, &req);
    return req;
  }

  template<typename T>
/**
 * @brief Passes data to the left neighbor in the pipeline.
 *
 * This function handles the communication with the previous rank in the pipeline,
 * sending and receiving data as necessary based on the current rank position.
 *
 * @param[in,out] reqs Vector of MPI requests.
 * @param[in] send_bulk Pointer to bulk data to be sent.
 * @param[in] send_last Pointer to last element to be sent.
 * @param[out] recv_first Pointer to first element to receive.
 * @param[out] recv_bulk Pointer to bulk data to receive.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void passLeft(std::vector<MPI_Request> &reqs,
		T const* send_bulk, T const *send_last,
		T* recv_first, T* recv_bulk) const{
    if(pipeline_depth == 1){
      *recv_first = *send_last; return;
    }
    
    if(is_last) reqs.push_back(send(*send_last, prev_rank));
    else if(!is_first) reqs.push_back(send(*send_bulk, prev_rank));

    if(is_first) reqs.push_back(recv(*recv_first, next_rank));
    else if(!is_last) reqs.push_back(recv(*recv_bulk, next_rank));
  }
  template<typename T>
/**
 * @brief Passes data to the right neighbor in the pipeline.
 *
 * This function handles the communication with the neighboring process
 * by sending and receiving data as necessary based on its position in the pipeline.
 *
 * @param[in,out] reqs vector of MPI requests
 * @param[in] send_first first element to be sent
 * @param[in] send_bulk bulk elements to be sent
 * @param[out] recv_bulk received bulk elements
 * @param[out] recv_last last received element
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void passRight(std::vector<MPI_Request> &reqs,
		T const* send_first, T const *send_bulk,
		T* recv_bulk, T* recv_last) const{
    if(pipeline_depth == 1){
      *recv_last = *send_first; return;
    }
    
    if(is_first) reqs.push_back(send(*send_first, next_rank));
    else if(!is_last) reqs.push_back(send(*send_bulk, next_rank));

    if(is_last) reqs.push_back(recv(*recv_last, prev_rank));
    else if(!is_first) reqs.push_back(recv(*recv_bulk, prev_rank));
  }

  template<typename T>
/**
 * @brief Shifts data from last element to first element in pipeline stage.
 *
 * @param[in,out] reqs vector of MPI requests
 * @param[in] send_last pointer to last element to be sent
 * @param[out] recv_first pointer to first element to receive data
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void passLeftLastToFirst(std::vector<MPI_Request> &reqs,
			   T const* send_last, T *recv_first){
    if(pipeline_depth == 1){
      *recv_first = *send_last; return;
    }
    if(is_last) reqs.push_back(send(*send_last,0));
    else if(is_first) reqs.push_back(recv(*recv_first,pipeline_depth-1));
  }

  int dcalls;
  
public:
  
/**
 * @brief Constructor for PipelineBlock object
 * 
 * Initializes the PipelineBlock with the given BlockStore, batch size, number of input features, 
 * number of features in this block, and number of features in the next block.
 * 
 * @param _block The BlockStore object to be used by the PipelineBlock
 * @param batch_size The size of the batches to be processed by the PipelineBlock
 * @param input_features The number of input features for the PipelineBlock
 * @param this_features The number of features in the current PipelineBlock
 * @param next_features The number of features in the next PipelineBlock
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  PipelineBlock(BlockStore &&_block, int batch_size, int input_features, int this_features, int next_features): block(std::move(_block)), batch_size(batch_size), input_features(input_features), this_features(this_features), next_features(next_features), dcalls(0){
    
    //Prepare rank information
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    
    int nranks;
    MPI_Comm_size(MPI_COMM_WORLD, &nranks);

    next_rank = rank+1;
    prev_rank = rank-1;
    if(next_rank == nranks) next_rank = -1;
    pipeline_depth = nranks;

    is_first = prev_rank == -1;
    is_last = next_rank == -1;

    //Compute parameter information
    std::vector<int> block_params(nranks, 0);
    block_params[rank] = block.v.nparams();
    
    MPI_Allreduce(MPI_IN_PLACE, block_params.data(), nranks, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    //Compute block param offset
    stage_off = 0;
    for(int i=0;i<rank;i++) stage_off += block_params[i];

    //Compute total params
    nparam = 0;
    for(int i=0;i<nranks;i++) nparam += block_params[i];
    
    //Setup storage
    prev_in = Matrix(this_features, batch_size, 0.);
    prev_above_deriv = Matrix(this_features, batch_size, 0.); //dcost/dout_i  from layer above
    prev_cost_deriv_passright = Vector(nparam,0.); //partially-populated cost deriv from layer above
    
    //Tell the block to resize its input buffers accordingly (cf below)
    block.v.resizeInputBuffer(2*rank+1);
  }

  PipelineBlock(const PipelineBlock &r) = delete;
  PipelineBlock(PipelineBlock &&r) = default;
  
  int nparams() const{ return nparam; }

  int pipelineDepth() const{ return pipeline_depth; }

  //Amount of iterations before you get the return value for the first item back
  int valueLag() const{ return pipeline_depth; }
  //Amount of iterations before you get the derivative for the first item back
  int derivLag() const{ return 2*pipeline_depth - 1; }

  //We assume every node in the group has access to the same x value called in the same order
  Matrix value(const Matrix &in){
    std::vector<MPI_Request> reqs;

    //<i- (<0-, <1- etc): item i in 'prev_in' at start of iteration, perform action and send left in this iter
    //<i|                : take item i from input 'in', perform action and send left in this iter

    //iter    rank->
    //        0     1      2
    //0                   <0|
    //1            <0-    <1|
    //2      <0-   <1-    <2|
    //3      <1-   <2-    <3|
    //etc

    Matrix out = block.v.value(is_last ? in : prev_in);
    passLeft(reqs,          &out,     &out,
	          &prev_in, &prev_in);    
    MPI_Waitall(reqs.size(), reqs.data(), MPI_STATUSES_IGNORE);
    
    return out; //note, output only meaningful on first node   
  }

/**
 * @brief Computes the derivatives of the cost with respect to the model parameters.
 *
 * This function takes as input the cost derivative and the derivative of the matrix above.
 * It computes the layer derivative and fills in the cost derivative vector.
 * The function also handles communication between ranks by sending and receiving data.
 *
 * @param[out] cost_deriv The cost derivative vector that will be updated.
 * @param[in] above_deriv The derivative of the matrix above.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void deriv(Vector &cost_deriv, const Matrix &above_deriv){
    ++dcalls;
    std::vector<MPI_Request> reqs;

    //For reverse differentiation we need to wait for a full value pipeline
    //As each layer needs the input value from the layer below to compute its derivative we need to buffer these appropriately

    //2 ranks
    //Value:
    //iter    rank->
    //        0     1
    //0            <0|
    //1      <0-   <1|
    //2      <1-   <2|
    //3      <2-   <3|
    //etc

    //value lag: 2
    
    //Deriv:
    //iter    rank->
    //        0     1    
    //0       
    //1      |0>   
    //2      |1>   -0>
    //3      |2>   -1>   
    //etc

    //deriv lag: 3
    
    //3 ranks
    //Value:
    //iter    rank->
    //        0     1      2
    //0                   <0|
    //1            <0-    <1|
    //2      <0-   <1-    <2|
    //3      <1-   <2-    <3|
    //etc
    
    //value lag: 3

    //Deriv:
    //iter    rank->
    //        0     1      2
    //0       
    //1       
    //2      |0>   
    //3      |1>   -0>
    //4      |2>   -1>    -0>    
    //etc

    //deriv lag: 5


    //4 ranks
    //Value:
    //iter    rank->
    //        0     1      2     3
    //0                         <0|
    //1                  <0-    <1|
    //2            <0-   <1-    <2|
    //3      <0-   <1-   <2-    <3|
    //4      <1-   <2-   <3-    <4|
    //etc
    
    //value lag: 4

    //Deriv:
    //iter    rank->
    //        0     1      2     3
    //0       
    //1
    //2
    //3      |0>   
    //4      |1>   -0>
    //5      |2>   -1>    -0>
    //6      |3>   -2>    -1>    -0>
    //7      |4>   -3>    -2>    -1>
    //etc

    //deriv lag: 7

    //value lag: nrank
    //deriv lag: 2*nrank - 1

    
    
    //To work out the buffering reqs we track what iter the derivs and values are computed for a particular item
    //For item 0:
    //Layer    Val-iter  Deriv-iter   
    // 0          2          2        
    // 1          1          3        
    // 2          0          4

    //For item 1:
    //Layer    Val-iter  Deriv-iter   
    // 0          3          3        
    // 1          2          4        
    // 2          1          5


    //Layer 0
    //Iter     Append    Consume
    // 0         
    // 1         
    // 2         0        0
    // 3         1        1
    // 4         2        2
    //lag = 0
    //buf_sz = 1
    
    //Layer 1
    //Iter     Append    Consume
    // 0         
    // 1         0
    // 2         1
    // 3         2         0
    // 4         3         1
    //lag = 2
    //buf_sz = 3
    
    //Layer 2
    //Iter     Append    Consume
    // 0         0
    // 1         1
    // 2         2
    // 3         3         
    // 4         4         0
    // 5         5         1
    //lag = 4
    //buf_sz = 5

    //buf_sz = 2*layer_idx + 1
    
    
    //compute layer derivative and fill in cost derivative vector
    //if this is the first rank, fill the input cost_deriv, else we append it to the deriv vector received last call
    Matrix layer_deriv(next_features, batch_size); //layer deriv to send right
    Vector pass_cost_deriv(is_first ? cost_deriv : prev_cost_deriv_passright); //cost deriv to send right

    //std::cout << "RANK " << rank << " CALL " << dcalls << " INPUT COST DERIV: " << pass_cost_deriv << " and base offset " << stage_off << std::endl;
    
    block.v.deriv(pass_cost_deriv, stage_off, is_first ? above_deriv : prev_above_deriv, &layer_deriv);

    //std::cout << "RANK " << rank << " CALL " << dcalls << " RESULT COST DERIV: " << pass_cost_deriv << std::endl;
    
    //send layer deriv to right if !last
    passRight(reqs, &layer_deriv, &layer_deriv,
                                 &prev_above_deriv, &prev_above_deriv); 
    
    //send new cost deriv to right if !last
    passRight(reqs, &pass_cost_deriv, &pass_cost_deriv,
	                           &prev_cost_deriv_passright, &prev_cost_deriv_passright);

    //if last our cost derivative is fully populated, so we send it back to rank 0 for output
    passLeftLastToFirst(reqs, &pass_cost_deriv, &cost_deriv);
    
    MPI_Waitall(reqs.size(), reqs.data(), MPI_STATUSES_IGNORE);

    //if(rank == 0) std::cout << "RANK 0 CALL " << dcalls << " RECEIVED COST DERIV: " << cost_deriv << std::endl;
  }

  // inline void update(int off, const Vector &new_params){}

  // inline void step(int off, const Vector &derivs, double eps){}
  
  // inline int nparams(){ return 0; }

  // inline void getParams(Vector &into, int off){}  
  
};

template<typename U, typename std::enable_if<ISLEAF(U), int>::type = 0>
auto pipeline_block(U &&u, int batch_size, int input_features, int this_features, int next_features)->PipelineBlock<DDST(u)>{
  return PipelineBlock<DDST(u)>(std::forward<U>(u),batch_size,input_features, this_features, next_features);
}


  
struct XYpair{
  Matrix x;
  Matrix y;
};


template<typename T, typename LRscheduler>
/**
 * @brief Optimizes a model using gradient descent with a specified learning rate schedule.
 * 
 * This function iterates over multiple epochs, shuffling the training data at each epoch,
 * computing the loss for each data point, and updating the model parameters accordingly.
 * 
 * @param model The model to be optimized.
 * @param data A vector of XY pairs representing the training data.
 * @param lr A learning rate scheduler object that provides the learning rate for each epoch.
 * @param nepoch The number of epochs to train the model for.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void optimizeGradientDescent(T &model, const std::vector<XYpair> &data, const LRscheduler &lr, int nepoch){
  std::default_random_engine gen(1234);
  std::uniform_int_distribution<int> dist(0,data.size());

  int ndata = data.size();
  
  std::vector<int> didx(ndata);
  for(int i=0;i<ndata;i++) didx[i] = i;
  
  for(int epoch=0;epoch<nepoch;epoch++){
    std::random_shuffle ( didx.begin(), didx.end(), [&](const int l){ return dist(gen); }  );
    double eps = lr(epoch);
    std::cout << "Epoch " << epoch << " learning rate " << eps << std::endl;
    
    for(int ii=0;ii<ndata;ii++){
      int i = didx[ii];
      double loss = model.loss(data[i].x, data[i].y);
      std::cout << epoch << "-" << ii << " : "<< loss << std::endl;
      model.step( model.deriv(), eps );
    }
  }

}



struct AdamParams{ //NB, alpha comes from the learning scheduler
  double beta1;
  double beta2;
  double eps;
  AdamParams( double beta1=0.99, double beta2=0.999, double eps=1e-8): beta1(beta1), beta2(beta2), eps(eps){}
};
  
template<typename T, typename LRscheduler>
/**
 * @brief Optimizes the given model using the Adam optimization algorithm.
 *
 * This function takes as input a model, training data, a learning rate scheduler,
 * Adam hyperparameters, and the number of epochs to train for. It then performs
 * stochastic gradient descent using the Adam optimizer to minimize the model's loss
 * function over the training data.
 *
 * @param model The model to be optimized.
 * @param data The training data used for optimization.
 * @param lr The learning rate scheduler.
 * @param ap The Adam hyperparameters.
 * @param nepoch The number of epochs to train for.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void optimizeAdam(T &model, const std::vector<XYpair> &data, const LRscheduler &lr, const AdamParams &ap, int nepoch){
  std::default_random_engine gen(1234);
  std::uniform_int_distribution<int> dist(0,data.size());

  int nparam = model.nparams();
  Vector m(nparam,0.0);
  Vector v(nparam,0.0);
  int t=0;
  
  int ndata = data.size();
  
  std::vector<int> didx(ndata);
  for(int i=0;i<ndata;i++) didx[i] = i;
  
  for(int epoch=0;epoch<nepoch;epoch++){
    std::random_shuffle ( didx.begin(), didx.end(), [&](const int l){ return dist(gen); }  );
    double alpha = lr(epoch);
    std::cout << "Epoch " << epoch << " learning rate " << alpha << std::endl;
    
    for(int ii=0;ii<ndata;ii++){
      int i = didx[ii];
      double loss = model.loss(data[i].x, data[i].y);
      auto g = model.deriv();

      double delta = t>0 ? alpha * sqrt(1. - pow(ap.beta2,t))  / (1. - pow(ap.beta1,t) ) : alpha;
      for(int p=0;p<nparam;p++){
	double gp_init = g(p);
	m(p) = ap.beta1 * m(p) + (1.-ap.beta1)*g(p);
	v(p) = ap.beta2 * v(p) + (1.-ap.beta2)*pow(g(p),2);

	g(p) = m(p)/(sqrt(v(p)) + ap.eps);
	//std::cout << "p="<< p << " m=" << m(p) << " v=" << v(p) << " g:" << gp_init << "->" <<  g(p) << std::endl;
      }
      ++t;      
      std::cout << epoch << "-" << ii << " : "<< loss << " update model with step size " << delta << std::endl;
      model.step( g , delta );
    }
  }

}


//TODO: Optimizer can be separate, needs to be passed just the gradient and return an ascent vector and step size
//TODO: Consider how to distribute layers over MPI. Each rank has a batch of layers. We need to keep every rank busy
//      Need distributed vectors and operations thereon

class DecayScheduler{
  double eps;
  double decay_rate;
public:
  DecayScheduler(double eps, double decay_rate): eps(eps), decay_rate(decay_rate){}
  double operator()(const int epoch) const{ return eps * 1./(1. + decay_rate * epoch); }
};


template<typename Store, typename ActivationFunc>
struct TestLeaf{
  Store leaf;
  ActivationFunc activation_func;
  typedef LeafTag tag;
  
  TestLeaf(Store &&leaf, const ActivationFunc &activation_func): leaf(std::move(leaf)), activation_func(activation_func){}
  TestLeaf(const TestLeaf &r) = delete;
  TestLeaf(TestLeaf &&r): leaf(std::move(r.leaf)){}
};
template<typename U>
auto test(U &&u)->TestLeaf<DDST(u),noActivation>{
  return TestLeaf<DDST(u),noActivation>(std::forward<U>(u),noActivation());
}


/**
 * @brief Performs basic tests for the neural network functionality.
 *
 * This function initializes a simple neural network with one layer, 
 * computes the mean squared error cost, and checks the derivative of 
 * the cost function with respect to the model parameters.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void basicTests(){
  typedef std::vector<double> vecD;
  
  Matrix w1_init(3,2, vecD({0.1,0.2,
	                   -0.1,-0.2,
			    0.7,0.7}));
  Vector b1_init( vecD({0.5,0.7,0.9}));		    
  
  auto f = mse_cost( dnn_layer(input_layer(), w1_init, b1_init) );

  //NB batch size 2, batches in different *columns*
  Matrix x1(2,2,vecD({1.3, 0.6,
	             -0.3, -1.7}));
  
  Matrix y1(3,2,std::vector<double>({-0.5, -0.5,
	                             1.7, 1.3
				     -0.7, -0.5}));

  double expect = 0.;
  for(int i=0;i<2;i++){  
    Vector y1pred = w1_init * x1.peekColumn(i) + b1_init;
    Vector y1_b = y1.peekColumn(i);
    expect += pow(y1pred(0)-y1_b(0),2)/3. + pow(y1pred(1)-y1_b(1),2)/3. + pow(y1pred(2)-y1_b(2),2)/3.;
  }
  expect /= 2.;
    
  double got=  f.loss(x1,y1);
  std::cout << "Test loss : got " << got << " expect " << expect << std::endl;


  Vector dexpect(9);
  int p=0;
  for(int i=0;i<3;i++){
    for(int j=0;j<2;j++){
      Matrix w1_p = w1_init;
      w1_p(i,j) += 1e-7;
      auto f2 = mse_cost( dnn_layer(input_layer(), w1_p, b1_init) );
      dexpect(p++) = (f2.loss(x1,y1) - got)/1e-7;
    }
  }
  for(int i=0;i<3;i++){
    Vector b1_p = b1_init;
    b1_p(i) += 1e-7;
    auto f2 = mse_cost( dnn_layer(input_layer(), w1_init, b1_p) );
    dexpect(p++) = (f2.loss(x1,y1) - got)/1e-7;    
  }

  Vector dgot = f.deriv();
  for(int i=0;i<9;i++){
    std::cout << "Test deriv wrt param " << i <<  ": got " << dgot(i) << " expect " << dexpect(i) << std::endl;
  }
    
  //test update
  Matrix w1_new(3,2, std::vector<double>({-0.5,0.4,
					  0.8,1.2,
					  2.1,-3.0}));
  Vector b1_new( std::vector<double>({-0.5,0.7,-1.1}));	

  auto ftest = mse_cost( dnn_layer(input_layer(), w1_new, b1_new) );
  f.update(ftest.getParams());
  
  std::cout << "Update check : expect " << ftest.loss(x1,y1) << " got " <<  f.loss(x1,y1) << std::endl;

}


/**
 * @brief Tests simple linear regression model with mean squared error cost function.
 *
 * This function generates synthetic data for a linear function,
 * initializes a neural network model with one layer, and trains the model
 * using gradient descent optimization algorithm with a decay scheduler.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testSimpleLinear(){
  //Test f(x) = 0.2*x + 0.3;

  Matrix winit(1,1,0.0);
  Vector binit(1,0.0);

  int ndata = 100;
  std::vector<XYpair> data(ndata);
  for(int i=0;i<ndata;i++){
    double eps = 2.0/(ndata - 1);
    double x = -1.0 + i*eps; //normalize x to within +-1

    data[i].x = Matrix(1,1,x);
    data[i].y = Matrix(1,1,0.2*x + 0.3);
  }
    
  auto model = mse_cost( dnn_layer(input_layer(), winit, binit) );
  DecayScheduler lr(0.01, 0.1);
  optimizeGradientDescent(model, data, lr, 200);

  std::cout << "Final params" << std::endl;
  Vector final_p = model.getParams();
  for(int i=0;i<final_p.size(0);i++)
    std::cout << i << " " << final_p(i) << std::endl;

}


/**
 * @brief Tests a neural network with one hidden layer.
 *
 * This function creates a simple dataset, initializes a neural network,
 * computes its derivatives, optimizes the parameters using Adam optimizer,
 * and tests the optimized model on the dataset.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testOneHiddenLayer(){
  //Test f(x) = 0.2*x + 0.3;
  int nbatch = 100;
  int batch_size = 4;
  std::vector<XYpair> data(nbatch);

  int ndata = batch_size * nbatch;

  for(int i=0;i<ndata;i++){ //i = b + batch_size * B
    double eps = 2.0/(ndata - 1);
    double x = -1.0 + i*eps; //normalize x to within +-1

    int b = i % batch_size;
    int B = i / batch_size;
    if(b==0){
      data[B].x = Matrix(1,batch_size);
      data[B].y = Matrix(1,batch_size);
    }
    
    data[B].x(0,b) = x;
    data[B].y(0,b) = 0.2*x + 0.3;
  }

  int nhidden = 5;

  Matrix winit_out(1,nhidden,0.01);
  Matrix winit_h(nhidden,1,0.01);

  Vector binit_out(1,0.01);
  Vector binit_h(nhidden, 0.01);

  auto hidden_layer( dnn_layer(input_layer(), winit_h, binit_h, ReLU()) );
  auto model = mse_cost( dnn_layer(hidden_layer, winit_out, binit_out) );

  //Test derivative
  {
    Vector p = model.getParams();
    
    for(int d=1;d<5;d++){ //first 5 data
    
      double c1 = model.loss(data[d].x,data[d].y);
      Vector pd = model.deriv();
      
      auto hidden_layer2 = dnn_layer(input_layer(), winit_h, binit_h, ReLU());  
      auto model2 = mse_cost( dnn_layer(hidden_layer2, winit_out, binit_out) );

      std::cout << "Test derivs " << d << " x=" << data[d].x(0,0) << " " << data[d].x(0,1) << std::endl;
      for(int i=0;i<p.size(0);i++){
	Vector pp(p);
	pp(i) += 1e-9;
	model2.update(pp);
      
	double c2 = model2.loss(data[d].x,data[d].y);
	std::cout << i << " got " << pd(i) << " expect " << (c2-c1)/1e-9 << std::endl;
      }
    }
  }


  DecayScheduler lr(0.001, 0.1);
  AdamParams ap;
  optimizeAdam(model, data, lr, ap, 200);

  std::cout << "Final params" << std::endl;
  Vector final_p = model.getParams();
  for(int i=0;i<final_p.size(0);i++)
    std::cout << i << " " << final_p(i) << std::endl;

  std::cout << "Test on some data" << std::endl;
  for(int d=0;d<data.size();d++){ //first 5 data, batch idx 0
    auto got = model.predict(data[d].x);
    std::cout << data[d].x(0,0) << " got " << got(0,0) << " expect " << data[d].y(0,0) << std::endl;
  }

}


//This functionality allows dynamic rather than compile time composition of layers
class LayerWrapperInternalBase{
public:
  virtual Matrix value(const Matrix &x) = 0;
  virtual void deriv(Vector &cost_deriv, int off, const Matrix &above_deriv, Matrix* input_above_deriv_copyback = nullptr) const = 0;
  virtual int nparams() const = 0;
  virtual ~LayerWrapperInternalBase(){}
};
template<typename Store, typename std::enable_if<ISSTORAGE(Store), int>::type = 0 >
class LayerWrapperInternal: public LayerWrapperInternalBase{
  Store layer;
public:
  LayerWrapperInternal(Store &&layer): layer(std::move(layer)){}
  
  Matrix value(const Matrix &x) override{
    return layer.v.value(x);
  }
  void deriv(Vector &cost_deriv, int off, const Matrix &above_deriv, Matrix* input_above_deriv_copyback = nullptr) const override{
    layer.v.deriv(cost_deriv,off,above_deriv, input_above_deriv_copyback);
  }
  int nparams() const{ return layer.v.nparams(); }
};
class LayerWrapper{
  std::unique_ptr<LayerWrapperInternalBase> layer;
public:
  typedef LeafTag tag;

  LayerWrapper(LayerWrapper &&r) = default;
  LayerWrapper & operator=(LayerWrapper &&r) = default;
  
  template<typename Store, typename std::enable_if<ISSTORAGE(Store), int>::type = 0 >
  LayerWrapper(Store &&layer): layer( new LayerWrapperInternal<Store>(std::move(layer)) ){}

  inline Matrix value(const Matrix &x){
    return layer->value(x);
  }
  inline void deriv(Vector &cost_deriv, int off, const Matrix &above_deriv, Matrix* input_above_deriv_copyback = nullptr) const{
    layer->deriv(cost_deriv,off,above_deriv, input_above_deriv_copyback);
  }
  inline int nparams() const{ return layer->nparams(); }
};

template<typename U, typename std::enable_if<ISLEAF(U), int>::type = 0>
LayerWrapper enwrap(U &&u){
  return LayerWrapper(DDST(u)(std::forward<U>(u)));
}



/**
 * @brief Tests the wrapping functionality of neural network layers.
 *
 * This function creates a deep neural network (DNN) model by composing multiple DNN layers,
 * then wraps the input layer with multiple DNN layers and compares their output values.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testWrapping(){
  double B=0;
  double A=2;
  Matrix winit(1,1,A);
  Vector binit(1,B);

  auto model = dnn_layer(
			 dnn_layer(
				   dnn_layer(
      				             input_layer(),
					     winit,binit),
				   winit, binit),
			 winit,binit);
 
  LayerWrapper composed = enwrap( input_layer() );
  for(int i=0;i<3;i++)
     composed = enwrap( dnn_layer(std::move(composed), winit,binit) );
 
  
  int iters=10;
  for(int i=0;i<iters;i++){
      Matrix x(1,1, i+1);
      Matrix vexpect = model.value(x);

      Matrix vgot = composed.value(x);

      std::cout << i << " got " << vgot(0,0) << " expect " << vexpect(0,0) << std::endl;
  }
}

/**
 * @brief Tests the functionality of the neural network pipeline.
 *
 * This function tests both the value and cost pipelines by comparing the results
 * obtained from the pipeline with expected values computed directly from the model.
 
void testPipeline(){
   * @brief Number of ranks in the communicator.
 
int nranks;
MPI_Comm_size(MPI_COMM_WORLD, &nranks);
 * @brief Rank of the current process.
 
int rank;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
   * @brief Batch size used for testing.
 
int batch_size = 1; * @brief Number of input features.
 
int input_features = 1;
  
 * @brief Constants used to initialize weights and biases.
 
double B=0.15;
double A=3.14;
   * @brief Initial weight matrix.
 
Matrix winit(1,1,A); * @brief Initial bias vector.
 
Vector binit(1,B);
typedef decltype( dnn_layer(input_layer(), winit,binit) ) Ltype;


if(1){ //test model
     * @brief Test whether we are running on the root rank.
   
  if(!rank) std::cout << "Testing model value pipeline" << std::endl;
     * @brief Create a pipeline block with the specified layer, batch size, input features, and rank-dependent flag.
   
  auto p = pipeline_block( dnn_layer(input_layer(), winit,binit), batch_size, input_features, 1, rank == nranks -1? 0 : 1);
  
     * @brief Get the value lag (number of iterations before the first complete cycle of forward differentiation).
   
  int value_lag = p.valueLag(); 
     * @brief Get the derivative lag (number of iterations before the first complete cycle of backward differentiation).
   
  int deriv_lag = p.derivLag();
  
     * @brief Number of iterations for testing.
   
  int iters=20;

     * @brief Build the same model on just this rank for comparison purposes.
   
  LayerWrapper test_model = enwrap( input_layer() );
  for(int r=0;r<nranks;r++) test_model = enwrap( dnn_layer(std::move(test_model), winit,binit) ); 

     * @brief Compute expected values and derivatives for comparison.
   
  if(!rank) std::cout << "Computing expectations" << std::endl;
  std::vector<Matrix> expect_v(iters);
  std::vector<Vector> expect_d(iters, Vector(test_model.nparams()) );
  
  std::vector<Matrix> input_deriv(iters);
  for(int i=0;i<iters;i++){
    input_deriv[i] = Matrix(1,batch_size, 2.13*(i+1)); 
    Matrix x(1,1, i+1);
    expect_v[i] = test_model.value(x);
    test_model.deriv(expect_d[i],0,input_deriv[i]);
  }
  int nparams = test_model.nparams();

     * @brief Start the test loop to compare pipeline results with expected values.
   
  if(!rank) std::cout << "Starting test loop" << std::endl;
  for(int i=0;i<iters;i++){
    Matrix x(1,1, i+1);
    Matrix v = p.value(x);
    Vector d(nparams,0.);

    int i_vpipe = i-(value_lag-1); 
    p.deriv(d,i_vpipe >= 0? input_deriv[i_vpipe] : Matrix(1,batch_size,-1)); 
    
    if(!rank){
      if(i_vpipe >=0 ){
        double ev = expect_v[i_vpipe](0,0); 
        std::cout << i << "\tval expect " << ev << " got "<<  v(0,0) << std::endl;
      }
      if(i_vpipe >=0 ){
        Vector ed = expect_d[i_vpipe];   
        std::cout << "\tderiv expect " << ed << " got " << d << std::endl;
      }
    }
  }
}
if(1){ //test cost
     * @brief Test the loss pipeline.
   
  if(!rank) std::cout << "Testing loss pipeline" << std::endl;
  auto p = pipeline_block( dnn_layer(input_layer(), winit,binit), batch_size, input_features, 1, rank == nranks -1? 0 : 1);
  PipelineCostFuncWrapper<decltype(p),MSEcostFunc> pc(p);
  int value_lag = p.valueLag();
  int deriv_lag = p.derivLag();
  
     * @brief Build the same model on just this rank for comparison purposes.
   
  LayerWrapper test_model = enwrap( input_layer() );
  for(int r=0;r<nranks;r++) test_model = enwrap( dnn_layer(std::move(test_model), winit,binit) ); 
  auto test_cost = mse_cost(test_model);

  int nparams = p.nparams();
  
  int iters=20;

  std::vector<Matrix> x(iters);
  std::vector<Matrix> y(iters);
  
  for(int i=0;i<iters;i++){
    x[i] = Matrix(1,1, i+1);

    double ival = i+1;
    for(int r=0;r<nranks;r++)
      ival = B + A*ival;

    y[i] = Matrix(1,1, 1.05*ival);
  }

     * @brief Compute expected losses and derivatives for comparison.
   
  if(!rank) std::cout << "Computing expectations" << std::endl;
  std::vector<double> expect_l(iters);
  std::vector<Vector> expect_d(iters, Vector(test_model.nparams()) );
  for(int i=0;i<iters;i++){
    expect_l[i] = test_cost.loss(x[i],y[i]);
    expect_d[i] = test_cost.deriv();
  }
  
     * @brief Start the test loop to compare pipeline results with expected values.
   
  if(!rank) std::cout << "Starting test loop" << std::endl;
  for(int i=0;i<iters;i++){
    int i_vpipe = i-(value_lag-1);
    double loss = pc.loss(x[i],y[i]);     
    double loss_expect = i_vpipe < 0? -1. : expect_l[i_vpipe];

    int i_dpipe = i-(deriv_lag-1); 
    Vector deriv = pc.deriv();
    Vector deriv_expect = i_dpipe < 0? Vector(nparams,-1.) : expect_d[i_dpipe];
    
    if(!rank){
      std::cout << i << "\tvalue expect " << loss_expect << " got "<<  loss << std::endl;
      std::cout << "\tderiv expect " << deriv_expect << " got " << deriv << std::endl;
    }
  }
}* This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testPipeline(){
  int nranks;
  MPI_Comm_size(MPI_COMM_WORLD, &nranks);

  int rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  
  int batch_size = 1;
  int input_features = 1;
  

  double B=0.15;
  double A=3.14;
  
  Matrix winit(1,1,A);
  Vector binit(1,B);
  typedef decltype( dnn_layer(input_layer(), winit,binit) ) Ltype;


  if(1){ //test model
    if(!rank) std::cout << "Testing model value pipeline" << std::endl;
    //auto b = dnn_layer(input_layer(), winit,binit);    
    //auto p = pipeline_block( b, batch_size, input_features, 1, rank == nranks -1 ? 0 : 1);

    auto p = pipeline_block( dnn_layer(input_layer(), winit,binit), batch_size, input_features, 1, rank == nranks -1 ? 0 : 1);
    
    int value_lag = p.valueLag(); //iterations before first complete cycle of forwards differentiation
    int deriv_lag = p.derivLag(); //iterations before first complete cycle of backwards differentiation
    
    int iters=20;

    //Build the same model on just this rank
    LayerWrapper test_model = enwrap( input_layer() );
    for(int r=0;r<nranks;r++) test_model = enwrap( dnn_layer(std::move(test_model), winit,binit) ); 

    if(!rank) std::cout << "Computing expectations" << std::endl;
    std::vector<Matrix> expect_v(iters);
    std::vector<Vector> expect_d(iters, Vector(test_model.nparams()) );
    
    std::vector<Matrix> input_deriv(iters);
    for(int i=0;i<iters;i++){
      input_deriv[i] = Matrix(1,batch_size, 2.13*(i+1)); 
      Matrix x(1,1, i+1);
      expect_v[i] = test_model.value(x);
      test_model.deriv(expect_d[i],0,input_deriv[i]);
    }
    int nparams = test_model.nparams();

    if(!rank) std::cout << "Starting test loop" << std::endl;
    for(int i=0;i<iters;i++){
      Matrix x(1,1, i+1);
      Matrix v = p.value(x);
      Vector d(nparams,0.);

      int i_vpipe = i-(value_lag-1); //lag=3    2->0  3->1
      int i_dpipe = i-(deriv_lag-1);
      p.deriv(d,i_vpipe >= 0 ? input_deriv[i_vpipe] : Matrix(1,batch_size,-1)); //use the input deriv appropriate to the item index!
      
      if(!rank){

	if(i_vpipe >=0 ){
	  double ev = expect_v[i_vpipe](0,0); 
	  std::cout << i << "\tval expect " << ev << " got "<<  v(0,0) << std::endl;
	}
	if(i_dpipe >=0 ){
	  Vector ed = expect_d[i_dpipe];	
	  std::cout << "\tderiv expect " << ed << " got " << d << std::endl;
	}
      }
    }
  }
  if(1){ //test cost
    if(!rank) std::cout << "Testing loss pipeline" << std::endl;
    auto p = pipeline_block( dnn_layer(input_layer(), winit,binit) , batch_size, input_features, 1, rank == nranks -1 ? 0 : 1);
    PipelineCostFuncWrapper<decltype(p),MSEcostFunc> pc(p);
    int value_lag = p.valueLag();
    int deriv_lag = p.derivLag();
    
    //Build the same model on just this rank
    LayerWrapper test_model = enwrap( input_layer() );
    for(int r=0;r<nranks;r++) test_model = enwrap( dnn_layer(std::move(test_model), winit,binit) ); 
    auto test_cost = mse_cost(test_model);

    int nparams = p.nparams();
    
    int iters=20;

    std::vector<Matrix> x(iters);
    std::vector<Matrix> y(iters);
    
    for(int i=0;i<iters;i++){
      x[i] = Matrix(1,1, i+1);

      double ival = i+1;
      for(int r=0;r<nranks;r++)
	ival = B + A*ival;

      //Add noise
      y[i] = Matrix(1,1, 1.05*ival);
    }

    //Get expectation loss and derivatives
    if(!rank) std::cout << "Computing expectations" << std::endl;
    std::vector<double> expect_l(iters);
    std::vector<Vector> expect_d(iters, Vector(test_model.nparams()) );
    for(int i=0;i<iters;i++){
      expect_l[i] = test_cost.loss(x[i],y[i]);
      expect_d[i] = test_cost.deriv();
    }
    
    if(!rank) std::cout << "Starting test loop" << std::endl;
    for(int i=0;i<iters;i++){
      int i_vpipe = i-(value_lag-1);
      double loss = pc.loss(x[i],y[i]);     
      double loss_expect = i_vpipe < 0 ? -1. : expect_l[i_vpipe];

      int i_dpipe = i-(deriv_lag-1); //item index associated with derivative
      Vector deriv = pc.deriv();
      Vector deriv_expect = i_dpipe < 0 ? Vector(nparams,-1.) : expect_d[i_dpipe];
      
      if(!rank){
	std::cout << i << "\tvalue expect " << loss_expect << " got "<<  loss << std::endl;
	std::cout << "\tderiv expect " << deriv_expect << " got " << deriv << std::endl;
      }
    }
  }

}


/**  * @brief Main program entry point
 * 
 * Initializes the Message Passing Interface and executes the test pipeline
 * 
 * @param argc Number of command line arguments
 * @param argv Array of command line argument strings
 * @return Program exit status
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
int main(int argc, char** argv){
  MPI_Init(&argc, &argv);
  
  //basicTests();
  //testSimpleLinear();
  //testOneHiddenLayer();
  //testWrapping();
  testPipeline();

  MPI_Finalize();
  return 0;
}

