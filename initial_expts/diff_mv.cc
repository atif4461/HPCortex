#include<iostream>
#include<cmath>
#include<vector>
#include <random>
#include <algorithm>
#include <cassert>

//First implementation of DNN with backwards differentiation including ADAM and gradient descent optimizers
struct LeafTag{};

struct Vector{
  std::vector<double> vals;
public:
  Vector(){}
  Vector(int size1): vals(size1){}
  Vector(int size1, double init): vals(size1, init){}
  Vector(const std::vector<double> &init_vals): vals(init_vals){}    
  
  inline double & operator()(const int i){ return vals[i]; }
  inline double operator()(const int i) const{ return vals[i]; }

  inline int size(int i) const{ return vals.size(); }
};

struct Matrix{
  std::vector<double> vals;
  int size1;
  int size2;
public:
  Matrix(int size1, int size2): size1(size1), size2(size2), vals(size1*size2){}  
  Matrix(int size1, int size2, double init): size1(size1), size2(size2), vals(size1*size2,init){}
  Matrix(int size1, int size2, const std::vector<double> &init_vals): size1(size1), size2(size2), vals(init_vals){}    
  
  inline double & operator()(const int i, const int j){ return vals[j+size2*i]; }
  inline double operator()(const int i, const int j) const{ return vals[j+size2*i]; }

  inline int size(int i) const{ return i==0 ? size1 : size2; }
};

/**
 * @brief Multiplies a matrix by a vector.
 * @param A The input matrix.
 * @param x The input vector.
 * @return The resulting vector from the multiplication.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
Vector operator*(const Matrix &A, const Vector &x){
  Vector out(A.size(0), 0.);
  for(int i=0;i<A.size(0);i++)
    for(int j=0;j<A.size(1);j++)
      out(i) += A(i,j) * x(j);
  return out;
}
/**
 * @brief Adds two vectors element-wise.
 *
 * This function performs component-wise addition of two input vectors,
 * returning a new vector containing the sum of corresponding elements.
 *
 * @param a The first vector operand.
 * @param b The second vector operand.
 * @return A new vector resulting from the element-wise addition of a and b.* This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
Vector operator+(const Vector &a, const Vector &b){
  Vector out(a.size(0));
  for(int i=0;i<a.size(0);i++)
    out(i) = a(i) + b(i);
  return out;
}
/**
 * @brief Subtracts two vectors element-wise.
 * @param a The first vector.
 * @param b The second vector.
 * @return A new vector containing the difference between corresponding elements of a and b.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
Vector operator-(const Vector &a, const Vector &b){
  Vector out(a.size(0));
  for(int i=0;i<a.size(0);i++)
    out(i) = a(i) - b(i);
  return out;
}
/**
 * @brief Scales a vector by a scalar value.
 * @param eps scaling factor
 * @param b input vector
 * @return scaled vector
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
Vector operator*(double eps, const Vector &b){
  Vector out(b.size(0));
  for(int i=0;i<b.size(0);i++)
    out(i) = eps * b(i);
}



class InputLayer{
  Vector val;
public:
  typedef LeafTag tag;
  
  InputLayer(){}
/**
 * @brief Returns a constant reference to the vector value.
 *
 * @param x The input vector.
 * @return A constant reference to the internal vector value.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  const Vector &value(const Vector &x){
    val = x;    
    return val;
  }

  void deriv(Vector &cost_deriv, int off, const Vector &above_deriv) const{ assert(off == cost_deriv.size(0)); }
  
  inline void update(int off, const Vector &new_params){}

  inline void step(int off, const Vector &derivs, double eps){}
  
  int nparams(){ return 0; }

  inline void getParams(Vector &into, int off){}    
};

InputLayer input_layer(){ return InputLayer(); }

template<typename T>
struct LeafStore{
  T v;
  LeafStore(T && v): v(v){
    //std::cout << "STORE" << std::endl;
  }
};
template<typename T>
struct LeafRef{
  T &v;
  LeafRef(T &v): v(v){
    //std::cout << "REF" << std::endl;
  }
};

template<typename T>
struct deduceStorage{};
template<typename T>
struct deduceStorage<T&>{
  typedef LeafRef<T> type;
};

template<typename T>
struct deduceStorage<T&&>{
  typedef LeafStore<T> type;
};

#define DDST(a) typename deduceStorage<decltype(a)>::type

#define ISLEAF(a) std::is_same<typename std::decay<a>::type::tag,LeafTag>::value

class ReLU{
public: 
/**
 * @brief Applies the ReLU activation function element-wise to the input vector.
 *
 * @param x Input vector to apply the ReLU function to.
 * @return Output vector with elements set to max(input, 0).
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  inline Vector operator()(const Vector &x) const{
    Vector out(x.size(0),1.0);
    //f(x)_i = max(x_i, 0)
    for(int i=0;i<x.size(0);i++) if(x(i) <= 0.) out(i) = 0.;
    return out;
  }
};

class noActivation{
public:
  inline Vector operator()(const Vector &x) const{
    return Vector(x.size(0),1.0);
  }
};

  


template<typename Store, typename ActivationFunc>
class DNNlayer{
  Matrix weights;
  Vector bias;  
  Store leaf;
  int size0;
  int size1;

  ActivationFunc activation_func;

  //Storage from last call to "value"
  Vector leaf_val;
  Vector activation;
public:
  typedef LeafTag tag;
  
/**
 * @brief Constructor for DNN layer initialization
 * @param leaf Store object to be moved
 * @param weights Weight matrix for neural network layer
 * @param bias Bias vector for neural network layer
 * @param activation_func Activation function used in the layer
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  DNNlayer(Store &&leaf, const Matrix &weights,const Vector &bias, const ActivationFunc &activation_func):
    leaf(leaf), weights(weights), bias(bias),
    size0(weights.size(0)), size1(weights.size(1)),
    activation_func(activation_func)
  {}

  //Forward pass
  Vector value(const Vector &x){
    leaf_val = leaf.v.value(x);
    assert(leaf_val.size(0) == size1);
    
    Vector out(bias);
    for(int i=0;i<size0;i++)
      for(int j=0;j<size1;j++)
	out(i) += weights(i,j)* leaf_val(j);
    
    activation = activation_func(out); assert(activation.size(0) == size0);
    for(int i=0;i<size0;i++) out(i) *= activation(i);
    
    return out;
  }
 
/**
 * Computes the derivative of the cost function with respect to the model parameters.
 *
 * @param[out] cost_deriv The vector to store the computed derivatives.
 * @param[in] off Offset index for storing the derivatives.
 * @param[in] above_deriv Derivative of the cost function with respect to the output of this layer.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void deriv(Vector &cost_deriv, int off, const Vector &above_deriv) const{
    assert(above_deriv.size(0) == size0);
    
    //for reverse differentiation, we pass down the derivatives with respect to our inputs
    //f(x)_i = act_i b_i + \sum_j act_i w_ij x_j
    //dcost / dx_j = \sum_i dcost/df_i df_i/dx_j
    //df_i/dx_j = act_i w_ij
    Vector layer_deriv(size1,0.);
    for(int j=0;j<size1;j++)    
      for(int i=0;i<size0;i++)
	layer_deriv(j) += above_deriv(i) * activation(i) * weights(i,j);

    //Now we finish up the derivs wrt our parameters
    //df(x)_i / d w_jk = delta_ij act_j x_k
    //df(x)_i / d b_j = delta_ij act_j
    //dcost / dw_jk = \sum_i dcost/df_i df_i/dw_jk = dcost/df_j * act_j * x_k
    //dcost / db_j = \sum_i dcost/df_i df_i/db_j = dcost/df_j * act_j
    int p=off;
    for(int j=0;j<size0;j++)
      for(int k=0;k<size1;k++)
	cost_deriv(p++) = above_deriv(j) * activation(j) * leaf_val(k);
    
    for(int j=0;j<size0;j++)
      cost_deriv(p++) = above_deriv(j) * activation(j);
    
    leaf.v.deriv(cost_deriv, p, layer_deriv);
  }

/**
 * @brief Updates model parameters at specified offset with new values.
 * @param off Offset where updates start.
 * @param new_params New parameter values to be applied.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void update(int off, const Vector &new_params){
    int p=off;
    for(int i=0;i<size0;i++)
      for(int j=0;j<size1;j++)
	weights(i,j) = new_params(p++);
    for(int i=0;i<size0;i++)
      bias(i) = new_params(p++);
    leaf.v.update(p, new_params);
  }
/**
 * @brief Updates model parameters based on computed derivatives and learning rate.
 * @param off Offset index for derivative array access.
 * @param derivs Array of computed derivatives.
 * @param eps Learning rate for parameter updates.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void step(int off, const Vector &derivs, double eps){
    int p=off;
    for(int i=0;i<size0;i++)
      for(int j=0;j<size1;j++){
	//std::cout << "Weights " << i << " " << j << " " << weights(i,j) << " -= " << derivs(p) << "*" << eps;
	weights(i,j) -= derivs(p++) * eps;
	//std::cout << " = " <<  weights(i,j) << std::endl;
      }
    for(int i=0;i<size0;i++){
      //std::cout << "Bias " << i << " " << bias(i) << " -= " << derivs(p) << "*" << eps;
      bias(i) -= derivs(p++) * eps;
      //std::cout << " = " << bias(i) << std::endl;
    }
    leaf.v.step(p, derivs, eps);
  }

  //accumulated #params for layers here and below
  inline int nparams(){ return size0*size1 + size0 + leaf.v.nparams(); }

  //off measured from *end*, return new off
  void getParams(Vector &into, int off){
    int p = off;
    for(int i=0;i<size0;i++)
      for(int j=0;j<size1;j++)
	into(p++) = weights(i,j);
    for(int i=0;i<size0;i++)
      into(p++) = bias(i);
    leaf.v.getParams(into, p);
  }
};

template<typename U, typename ActivationFunc, typename std::enable_if<ISLEAF(U), int>::type = 0>
auto dnn_layer(U &&u, const Matrix &weights,const Vector &bias, const ActivationFunc &activation)->DNNlayer<DDST(u),ActivationFunc>{
  return DNNlayer<DDST(u),ActivationFunc>(std::forward<U>(u), weights, bias, activation);
}
template<typename U, typename std::enable_if<ISLEAF(U), int>::type = 0>
auto dnn_layer(U &&u, const Matrix &weights,const Vector &bias)->DNNlayer<DDST(u),noActivation>{
  return DNNlayer<DDST(u),noActivation>(std::forward<U>(u), weights, bias, noActivation());
}


template<typename Store>
class MSEcost{
  Store leaf;
  Vector ypred;
  Vector yval;
  int nparam;
public:
  MSEcost(Store &&leaf): leaf(leaf), nparam(leaf.v.nparams()){}

/**
 * @brief Calculates the mean squared error between predicted and actual values.
 *
 * This function computes the average of the sum of squared differences between
 * corresponding elements of two vectors.
 *
 * @param x Input vector used to compute predicted values.
 * @param y Actual output vector.
 * @return Mean squared error value.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  double loss(const Vector &x, const Vector &y){
    //out = \sum_j (ypred(j) - y(j))^2/dim
    ypred = leaf.v.value(x);
    int dim = y.size(0);
    assert(ypred.size(0) == dim);
    yval = y;
    double out = 0.;
    for(int i=0;i<dim;i++)
      out += pow(ypred(i) - y(i),2);
    out /= dim;
    return out;
  }
  Vector predict(const Vector &x){
    return leaf.v.value(x);
  }
 
/**
 * @brief Computes the derivative of the cost function with respect to model parameters.
 *
 * This method calculates the gradient of the loss function by backpropagating the error through the network,
 * utilizing the chain rule to compute the partial derivatives of the output with respect to each parameter.
 *
 * @return A vector containing the derivatives of the cost function with respect to each model parameter.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  Vector deriv() const{
    //for reverse differentiation, we pass down the derivatives with respect to our inputs
    //dout / dparam(i) = \sum_j 2*(ypred(j) - y(j)) * dypred(j)/dparam(i)

    //dout / dypred(i) = 2*(ypred(i) - y(i)) /dim
    int dim = yval.size(0);
    Vector layer_deriv(dim);
    for(int i=0;i<dim;i++) layer_deriv(i) = 2*(ypred(i) - yval(i)) / dim;

    Vector cost_deriv(nparam,0.);    
    leaf.v.deriv(cost_deriv, 0, layer_deriv);
    return cost_deriv;
  }
  
  void update(const Vector &new_params){
    leaf.v.update(0, new_params);
  }
  void step(const Vector &derivs, double eps){
    leaf.v.step(0,derivs,eps);
  }
  int nparams(){ return nparam; }

/**
 * @brief Retrieves model parameters
 * @return Vector containing model parameters
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  Vector getParams(){
    Vector out(nparams());
    leaf.v.getParams(out,0);
    return out;
  }
};

template<typename U, typename std::enable_if<ISLEAF(U), int>::type = 0>
auto mse_cost(U &&u)->MSEcost<DDST(u)>{
  return MSEcost<DDST(u)>(std::forward<U>(u));
}

struct XYpair{
  Vector x;
  Vector y;
};


template<typename T, typename LRscheduler>
/**
 * @brief Optimizes a model using gradient descent algorithm.
 *
 * This function optimizes a given model by iteratively updating its parameters
 * using gradient descent with a specified learning rate schedule.
 *
 * @param model The model to be optimized.
 * @param data Training data used for optimization.
 * @param lr Learning rate scheduler.
 * @param nepoch Number of epochs for training.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void optimizeGradientDescent(T &model, const std::vector<XYpair> &data, const LRscheduler &lr, int nepoch){
  std::default_random_engine gen(1234);
  std::uniform_int_distribution<int> dist(0,data.size());

  int ndata = data.size();
  
  std::vector<int> didx(ndata);
  for(int i=0;i<ndata;i++) didx[i] = i;
  
  for(int epoch=0;epoch<nepoch;epoch++){
    std::random_shuffle ( didx.begin(), didx.end(), [&](const int l){ return dist(gen); }  );
    double eps = lr(epoch);
    std::cout << "Epoch " << epoch << " learning rate " << eps << std::endl;
    
    for(int ii=0;ii<ndata;ii++){
      int i = didx[ii];
      double loss = model.loss(data[i].x, data[i].y);
      std::cout << epoch << "-" << ii << " : "<< loss << std::endl;
      model.step( model.deriv(), eps );
    }
  }

}



struct AdamParams{ //NB, alpha comes from the learning scheduler
  double beta1;
  double beta2;
  double eps;
  AdamParams( double beta1=0.99, double beta2=0.999, double eps=1e-8): beta1(beta1), beta2(beta2), eps(eps){}
};
  
template<typename T, typename LRscheduler>
/**
 * @brief Optimizes the given model using the Adam optimization algorithm.
 *
 * This function takes in a model, training data, learning rate scheduler,
 * Adam hyperparameters, and number of epochs as input. It then performs
 * stochastic gradient descent using the Adam optimizer to minimize the
 * model's loss function over the specified number of epochs.
 *
 * @param model The model to be optimized.
 * @param data The training data used for optimization.
 * @param lr The learning rate scheduler.
 * @param ap The Adam hyperparameters.
 * @param nepoch The number of epochs to perform optimization for.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void optimizeAdam(T &model, const std::vector<XYpair> &data, const LRscheduler &lr, const AdamParams &ap, int nepoch){
  std::default_random_engine gen(1234);
  std::uniform_int_distribution<int> dist(0,data.size());

  int nparam = model.nparams();
  Vector m(nparam,0.0);
  Vector v(nparam,0.0);
  int t=0;
  
  int ndata = data.size();
  
  std::vector<int> didx(ndata);
  for(int i=0;i<ndata;i++) didx[i] = i;
  
  for(int epoch=0;epoch<nepoch;epoch++){
    std::random_shuffle ( didx.begin(), didx.end(), [&](const int l){ return dist(gen); }  );
    double alpha = lr(epoch);
    std::cout << "Epoch " << epoch << " learning rate " << alpha << std::endl;
    
    for(int ii=0;ii<ndata;ii++){
      int i = didx[ii];
      double loss = model.loss(data[i].x, data[i].y);
      auto g = model.deriv();

      double delta = t>0 ? alpha * sqrt(1. - pow(ap.beta2,t))  / (1. - pow(ap.beta1,t) ) : alpha;
      for(int p=0;p<nparam;p++){
	double gp_init = g(p);
	m(p) = ap.beta1 * m(p) + (1.-ap.beta1)*g(p);
	v(p) = ap.beta2 * v(p) + (1.-ap.beta2)*pow(g(p),2);

	g(p) = m(p)/(sqrt(v(p)) + ap.eps);
	//std::cout << "p="<< p << " m=" << m(p) << " v=" << v(p) << " g:" << gp_init << "->" <<  g(p) << std::endl;
      }
      ++t;      
      std::cout << epoch << "-" << ii << " : "<< loss << " update model with step size " << delta << std::endl;
      model.step( g , delta );
    }
  }

}


//TODO: Optimizer can be separate, needs to be passed just the gradient and return an ascent vector and step size
//TODO: Consider how to distribute layers over MPI. Each rank has a batch of layers. We need to keep every rank busy
//      Need distributed vectors and operations thereon

class DecayScheduler{
  double eps;
  double decay_rate;
public:
  DecayScheduler(double eps, double decay_rate): eps(eps), decay_rate(decay_rate){}
  double operator()(const int epoch) const{ return eps * 1./(1. + decay_rate * epoch); }
};


/**
 * @brief Performs basic tests for the neural network functionality.
 *
 * This function initializes a simple neural network with predefined weights and biases,
 * computes the mean squared error cost, and checks the correctness of the computed loss value.
 * It also verifies the derivative of the loss with respect to the model parameters and updates the model parameters.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void basicTests(){
  Matrix w1_init(3,2, std::vector<double>({0.1,0.2,
					  -0.1,-0.2,
					  0.7,0.7}));
  Vector b1_init( std::vector<double>({0.5,0.7,0.9}));		    
  
  auto f = mse_cost( dnn_layer(input_layer(), w1_init, b1_init) );

  Vector x1(std::vector<double>({1.3,-0.3}));
  Vector y1(std::vector<double>({-0.5,1.7,-0.7}));
  
  Vector y1pred = w1_init * x1 + b1_init;
  double expect = pow(y1pred(0)-y1(0),2)/3. + pow(y1pred(1)-y1(1),2)/3. + pow(y1pred(2)-y1(2),2)/3.;
  double got=  f.loss(x1,y1);
  std::cout << "Test loss : got " << got << " expect " << expect << std::endl;

  Vector dexpect(9);
  int p=0;
  for(int i=0;i<3;i++){
    for(int j=0;j<2;j++){
      Matrix w1_p = w1_init;
      w1_p(i,j) += 1e-7;
      auto f2 = mse_cost( dnn_layer(input_layer(), w1_p, b1_init) );
      dexpect(p++) = (f2.loss(x1,y1) - got)/1e-7;
    }
  }
  for(int i=0;i<3;i++){
    Vector b1_p = b1_init;
    b1_p(i) += 1e-7;
    auto f2 = mse_cost( dnn_layer(input_layer(), w1_init, b1_p) );
    dexpect(p++) = (f2.loss(x1,y1) - got)/1e-7;    
  }

  Vector dgot = f.deriv();
  for(int i=0;i<9;i++){
    std::cout << "Test deriv wrt param " << i <<  ": got " << dgot(i) << " expect " << dexpect(i) << std::endl;
  }
    
  //test update
  Matrix w1_new(3,2, std::vector<double>({-0.5,0.4,
					  0.8,1.2,
					  2.1,-3.0}));
  Vector b1_new( std::vector<double>({-0.5,0.7,-1.1}));	

  auto ftest = mse_cost( dnn_layer(input_layer(), w1_new, b1_new) );
  f.update(ftest.getParams());
  
  std::cout << "Update check : expect " << ftest.loss(x1,y1) << " got " <<  f.loss(x1,y1) << std::endl;
}

/**
 * @brief Tests simple linear regression model with mean squared error cost function.
 *
 * This function generates synthetic data for a linear function,
 * initializes a neural network model with one layer, and trains the model
 * using gradient descent optimization algorithm with a decay scheduler.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testSimpleLinear(){
  //Test f(x) = 0.2*x + 0.3;

  Matrix winit(1,1,0.0);
  Vector binit(1,0.0);

  int ndata = 100;
  std::vector<XYpair> data(ndata);
  for(int i=0;i<ndata;i++){
    double eps = 2.0/(ndata - 1);
    double x = -1.0 + i*eps; //normalize x to within +-1
    
    data[i].x = Vector(1,x);
    data[i].y = Vector(1,0.2*x + 0.3);
  }
    
  auto model = mse_cost( dnn_layer(input_layer(), winit, binit) );
  DecayScheduler lr(0.01, 0.1);
  optimizeGradientDescent(model, data, lr, 200);

  std::cout << "Final params" << std::endl;
  Vector final_p = model.getParams();
  for(int i=0;i<final_p.size(0);i++)
    std::cout << i << " " << final_p(i) << std::endl;

}


/**
 * @brief Tests a neural network with one hidden layer.
 *
 * This function creates a simple dataset, initializes a neural network with one hidden layer,
 * and tests its performance by comparing calculated derivatives with finite differences.
 * It also trains the network using the Adam optimizer and prints out the final parameters.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testOneHiddenLayer(){
  //Test f(x) = 0.2*x + 0.3;
  int ndata = 100;
  std::vector<XYpair> data(ndata);
  for(int i=0;i<ndata;i++){
    double eps = 2.0/(ndata - 1);
    double x = -1.0 + i*eps; //normalize x to within +-1
    
    data[i].x = Vector(1,x);
    data[i].y = Vector(1,0.2*x + 0.3);
  }

  int nhidden = 5;

  Matrix winit_out(1,nhidden,0.01);
  Matrix winit_h(nhidden,1,0.01);

  Vector binit_out(1,0.01);
  Vector binit_h(nhidden, 0.01);

  auto hidden_layer = dnn_layer(input_layer(), winit_h, binit_h, ReLU());
  auto model = mse_cost( dnn_layer(hidden_layer, winit_out, binit_out) );

  //Test derivative
  {
    Vector p = model.getParams();
    
    for(int d=1;d<5;d++){ //first 5 data
    
      double c1 = model.loss(data[d].x,data[d].y);
      Vector pd = model.deriv();
      
      auto hidden_layer2 = dnn_layer(input_layer(), winit_h, binit_h, ReLU());  
      auto model2 = mse_cost( dnn_layer(hidden_layer2, winit_out, binit_out) );

      std::cout << "Test derivs x=" << data[d].x(0) << std::endl;
      for(int i=0;i<p.size(0);i++){
	Vector pp(p);
	pp(i) += 1e-9;
	model2.update(pp);
      
	double c2 = model2.loss(data[d].x,data[d].y);
	std::cout << i << " got " << pd(i) << " expect " << (c2-c1)/1e-9 << std::endl;
      }
    }
  }

#if 1
  DecayScheduler lr(0.001, 0.1);
  AdamParams ap;
  optimizeAdam(model, data, lr, ap, 200);

  std::cout << "Final params" << std::endl;
  Vector final_p = model.getParams();
  for(int i=0;i<final_p.size(0);i++)
    std::cout << i << " " << final_p(i) << std::endl;

  std::cout << "Test on some data" << std::endl;
  for(int d=0;d<data.size();d++){ //first 5 data    
    auto got = model.predict(data[d].x);
    std::cout << data[d].x(0) << " got " << got(0) << " expect " << data[d].y(0) << std::endl;
  }
#endif
}

/**
 * @brief Program entry point
 * @return Integer indicating program execution status
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
int main(void){
  //basicTests();
  //testSimpleLinear();
  testOneHiddenLayer();

  return 0;
}

  
