#include <HPCortex.hpp>
#include <Testing.hpp>

template<typename _FloatType, int Dim>
struct SoftMaxComponentWrapper{
  typedef _FloatType FloatType;
  
  SoftMaxComponent<FloatType,Dim> &cpt;
  int size[Dim];
  size_t size_lin;

/**
 * @brief Constructor for wrapping SoftMaxComponent with additional functionality
 * @param cpt SoftMaxComponent object to be wrapped
 * @param sz Array of integers representing component sizes
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  SoftMaxComponentWrapper(SoftMaxComponent<FloatType,Dim> &cpt, int const *sz): cpt(cpt){
    memcpy(size,sz,Dim*sizeof(int));
    size_lin = 1;
    for(int d=0;d<Dim;d++) size_lin *= sz[d];
  }

  size_t outputLinearSize() const{ return size_lin; }
  size_t inputLinearSize() const{ return size_lin; }
  
/**
 * @brief Computes the value of a tensor transformation based on input vector.
 *
 * @param[in] in Input vector to be transformed.
 * @return Transformed output vector.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  Vector<FloatType> value(const Vector<FloatType> &in){
    Tensor<FloatType,Dim> A(size);
    unflatten(A,in);
    Tensor<FloatType,Dim> C = cpt.value(A);
    return flatten(C);
  }
/**
 * @brief Computes derivatives of cost with respect to input parameters.
 *
 * This function calculates the partial derivatives of the cost function 
 * with respect to the input parameters, utilizing previously computed 
 * derivatives from higher levels of the computation hierarchy.
 *
 * @param[out] cost_deriv_params 
 * @param[in] off 
 * @param[in] _above_deriv_lin Linearized derivative information from above level
 * @param[out] cost_deriv_inputs Derivative of cost with respect to inputs
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  void deriv(Vector<FloatType> &cost_deriv_params, int off, Vector<FloatType> &&_above_deriv_lin, Vector<FloatType> &cost_deriv_inputs){
    Vector<FloatType> above_deriv_lin = std::move(_above_deriv_lin);
    Tensor<FloatType,Dim> above_deriv(size);
    unflatten(above_deriv,above_deriv_lin);
    Tensor<FloatType,Dim> dcost_by_dIn;
    cpt.deriv(std::move(above_deriv), dcost_by_dIn);
    cost_deriv_inputs = flatten(dcost_by_dIn);
  }
    
  void update(int off, const Vector<FloatType> &new_params){}
  void step(int off, const Vector<FloatType> &derivs, FloatType eps){}
  inline int nparams() const{ return cpt.nparams(); }
  void getParams(Vector<FloatType> &into, int off){}

/**
 * @brief Returns a string representation of the coordinates corresponding to the given index.
 *
 * @param[in] i The index to be converted to coordinates.
 * @return A string representing the coordinates in the format "(x, y,..., z)".
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
  std::string inCoord(size_t i) const{
    std::ostringstream ss;
    int coord[Dim];
    tensorOffsetUnmap<Dim>(coord, size, i);
    ss << "(";
    for(int d=0;d<Dim;d++)
      ss << coord[d] << (d==Dim-1? ")" : ", ");
    return ss.str();
  }       
};

/**
 * @brief Tests the functionality of the SoftMaxComponent class.
 *
 * This function tests the SoftMaxComponent class by creating instances with different dimensions,
 * computing their values, and comparing them to expected results.
 
void testSoftMaxComponent() 
 * @brief Computes the value of the SoftMaxComponent instance.
 *
 * This function creates a SoftMaxComponent instance with the specified dimension and beta value,
 * computes its value for a given input tensor, and compares it to the expected result.
 *
 * @param logp Input tensor
 * @return Computed value of the SoftMaxComponent instance
 
Matrix<FloatType> SoftMaxComponent<FloatType,2>::value(Matrix<FloatType> logp)
 * @brief Computes the value of the SoftMaxComponent instance.
 *
 * This function creates a SoftMaxComponent instance with the specified dimension and beta value,
 * computes its value for a given input tensor, and compares it to the expected result.
 *
 * @param logp Input tensor
 * @return Computed value of the SoftMaxComponent instance
 
Tensor<FloatType,3> SoftMaxComponent<FloatType,3>::value(Tensor<FloatType,3> logp)
 * @brief Checks if two tensors are approximately equal within a certain tolerance.
 *
 * This function checks if the absolute difference between corresponding elements of two tensors
 * is less than a specified tolerance.
 *
 * @param vgot First tensor
 * @param vexpect Second tensor
 * @param tol Tolerance
 * @param verbose Flag to print comparison results
 * @return True if the tensors are approximately equal, false otherwise
 
bool absNear(Matrix<FloatType> vgot, Matrix<FloatType> vexpect, FloatType tol, bool verbose)
 * @brief Checks if two tensors are approximately equal within a certain tolerance.
 *
 * This function checks if the absolute difference between corresponding elements of two tensors
 * is less than a specified tolerance.
 *
 * @param vgot First tensor
 * @param vexpect Second tensor
 * @param tol Tolerance
 * @param verbose Flag to print comparison results
 * @return True if the tensors are approximately equal, false otherwise
 
bool absNear(Tensor<FloatType,3> vgot, Tensor<FloatType,3> vexpect, FloatType tol, bool verbose)
 * @brief Performs an action on each element of a tensor.
 *
 * This function applies a lambda function to each element of a tensor.
 *
 * @param vexpect Output tensor
 * @param vgot Input tensor
 * @param logp Input tensor
 * @param func Lambda function to apply
 
void doHost3(Tensor<FloatType,3>& vexpect, const Tensor<FloatType,3>& vgot, const Tensor<FloatType,3>& logp, const std::function<void()> func)
 * @brief Wraps a SoftMaxComponent instance for testing purposes.
 *
 * This wrapper provides additional functionality for testing the SoftMaxComponent class.
 
template <typename FloatType, int Dim>
class SoftMaxComponentWrapper {
public:
         * @brief Constructor.
     *
     * Initializes the wrapper with a SoftMaxComponent instance and tensor sizes.
     *
     * @param cpt SoftMaxComponent instance
     * @param size Tensor sizes
     
    SoftMaxComponentWrapper(SoftMaxComponent<FloatType,Dim> cpt, int size[Dim])
}
 * @brief Tests the derivative of a component.
 *
 * This function tests the derivative of a component by comparing computed and analytical derivatives.
 *
 * @param wrp Component wrapper
 
void testComponentDeriv(SoftMaxComponentWrapper<FloatType,4>& wrp)* This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testSoftMaxComponent(){
  typedef double FloatType;
  FloatType delta = 1e-4;
  std::mt19937 rng(1234);
   
  typedef std::vector<FloatType> vecD;

  FloatType beta = 0.3;

  {
    int size[2] = {4,5};
    Matrix<FloatType> logp(size);
    random(logp,rng);

    SoftMaxComponent<FloatType,2> cpt(0, beta);

    Matrix<FloatType> vgot = cpt.value(logp);
    Matrix<FloatType> vexpect(size);

    doHost3(vexpect, vgot, logp, {
	std::vector<FloatType> logp_pencil(size[0]);
	for(int b=0;b<size[1];b++){
	  for(int i=0;i<size[0];i++)
	    logp_pencil[i] = logp_v(i,b);
	  std::vector<FloatType> expect_pencil = softMaxVector(logp_pencil, beta);
	  for(int i=0;i<size[0];i++){
	    vexpect_v(i,b) = expect_pencil[i];
	    std::cout << i << " " << b << " got " << vgot_v(i,b) << " expect " << vexpect_v(i,b) << std::endl;
	  }
	}
      });
    
    assert(abs_near(vgot,vexpect,FloatType(1e-4),true));
  }
  {
    int size[3] = {3,4,5};
    Tensor<FloatType,3> logp(size);
    random(logp,rng);

    {//dim 0
      SoftMaxComponent<FloatType,3> cpt(0, beta);

      Tensor<FloatType,3> vgot = cpt.value(logp);
      Tensor<FloatType,3> vexpect(size);

      doHost3(vexpect, vgot, logp, {
	  std::vector<FloatType> logp_pencil(size[0]);
	  for(int j=0;j<size[1];j++){
	    for(int b=0;b<size[2];b++){
	      
	      for(int i=0;i<size[0];i++)
		logp_pencil[i] = logp_v(i,j,b);
	      std::vector<FloatType> expect_pencil = softMaxVector(logp_pencil, beta);
	      for(int i=0;i<size[0];i++){
		vexpect_v(i,j,b) = expect_pencil[i];
		std::cout << i << " " << j << " " << b << " got " << vgot_v(i,j,b) << " expect " << vexpect_v(i,j,b) << std::endl;
	      }
	      
	    }
	  }
	});
    
      assert(abs_near(vgot,vexpect,FloatType(1e-4),true));
    }
    {//dim 1
      SoftMaxComponent<FloatType,3> cpt(1, beta);

      Tensor<FloatType,3> vgot = cpt.value(logp);
      Tensor<FloatType,3> vexpect(size);

      doHost3(vexpect, vgot, logp, {
	  std::vector<FloatType> logp_pencil(size[1]);
	  for(int i=0;i<size[0];i++){
	    for(int b=0;b<size[2];b++){
	      
	      for(int j=0;j<size[1];j++)
		logp_pencil[j] = logp_v(i,j,b);
	      std::vector<FloatType> expect_pencil = softMaxVector(logp_pencil, beta);
	      for(int j=0;j<size[1];j++){
		vexpect_v(i,j,b) = expect_pencil[j];
		std::cout << i << " " << j << " " << b << " got " << vgot_v(i,j,b) << " expect " << vexpect_v(i,j,b) << std::endl;
	      }
	      
	    }
	  }
	});
    
      assert(abs_near(vgot,vexpect,FloatType(1e-4),true));
    }

      
  }

  //Check derivatives
  for(int d=0;d<3;d++){
    std::cout << "Testing derivs for softmax on dim " << d << std::endl;
    int size[4] = {2,3,4,5};
    SoftMaxComponent<FloatType,4> cpt(d, beta);
    SoftMaxComponentWrapper<FloatType,4> wrp(cpt,size);
    testComponentDeriv(wrp);
  }
    
}


/**
 * @brief Tests the functionality of the softmax layer.
 *
 * This function tests both the forward pass and backward pass of the softmax layer.
 * It checks if the output values match the expected results within a certain tolerance.
 * Additionally, it verifies that the input derivatives calculated by the softmax layer
 * are correct by comparing them with numerically computed derivatives.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testSoftMaxLayer(){
  typedef float FloatType;
  FloatType delta = 1e-4;
  std::mt19937 rng(1234);
   
  typedef std::vector<FloatType> vecD;

  FloatType beta = 0.3;
  auto m = softmax_layer(input_layer<FloatType>(), beta);

  int np = 20;
  int batch_size = 5;
  
  Matrix<FloatType> logp(np,batch_size);
  random(logp,rng);

  ///value
  Matrix<FloatType> vgot = m.value(logp);

  Matrix<FloatType> vexpect(np,batch_size);
  doHost3(vexpect, vgot, logp, {
      for(int b=0;b<batch_size;b++){
	FloatType max = logp_v(0,b);
	for(int i=1;i<np;i++)
	  max = std::max(max, logp_v(i,b));

	FloatType norm = exp(beta*(logp_v(0,b)-max));
	for(int i=1;i<np;i++)
	  norm += exp(beta*(logp_v(i,b)-max));
	
	for(int i=0;i<np;i++){
	  vexpect_v(i,b) = exp(beta*(logp_v(i,b)-max)) / norm;
	  std::cout << i << " " << b << " got " << vgot_v(i,b) << " expect " << vexpect_v(i,b) << std::endl;
	}
      }
    });
    
  assert(abs_near(vgot,vexpect,FloatType(1e-4),true));

  //deriv
  //it has no parameters so we only need to test the input derivatives
  Matrix<FloatType> in_deriv; //dcost/din_j
  Vector<FloatType> cost_deriv_dummy;

  //let  cost = \sum_i c_i * out_i
  //above_deriv = dcost/dout_i = c_i
  Vector<FloatType> c(np);
  random(c,rng);
    
  Matrix<FloatType> above_deriv(np,batch_size); //dcost/dout_i
  doHost2(above_deriv, c, {
      for(int i=0;i<np;i++)
	for(int b=0;b<batch_size;b++)
	  above_deriv_v(i,b) = c_v(i);
    });
  m.deriv(cost_deriv_dummy, 0, std::move(above_deriv), &in_deriv);

  Matrix<FloatType> expect_deriv(np,batch_size, 0.);
  
  for(int j=0;j<np;j++){
    Matrix<FloatType> logp_dj = logp;
    doHost(logp_dj, {
	for(int b=0;b<batch_size;b++)
	  logp_dj_v(j,b) += delta;
      });
    Matrix<FloatType> vp = m.value(logp_dj); //out_i( ... in_j+delta ... )
    Matrix<FloatType> dout_dinj = (FloatType(1.)/delta) * (vp - vgot);
    doHost3(expect_deriv,dout_dinj,c,
	    {
	      for(int i=0;i<np;i++)
		for(int b=0;b<batch_size;b++)
		  expect_deriv_v(j,b) += c_v(i)*dout_dinj_v(i,b);
	    });
  }
  assert(abs_near(in_deriv, expect_deriv, FloatType(1e-3), true));
  
  std::cout << "Tests passed" << std::endl;
}

/**
 * @brief Program entry point
 * @param argc Number of command line arguments
 * @param argv Array of command line argument strings
 * @return Exit status of the program
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
int main(int argc, char** argv){
  initialize(argc,argv);
  testSoftMaxComponent();
  testSoftMaxLayer();

  return 0;
}
