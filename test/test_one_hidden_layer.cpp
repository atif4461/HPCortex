#include <HPCortex.hpp>
#include <Testing.hpp>

/**
 * @brief Tests a neural network with one hidden layer.
 *
 * This function creates a simple neural network with one hidden layer,
 * trains it on a synthetic dataset, and tests its performance.
 
void testOneHiddenLayer(){
   * @brief Number of batches in the training dataset.
 
int nbatch = 100;
   * @brief Size of each batch in the training dataset.
 
int batch_size = 4;
 * @brief Data type used for floating point numbers.
 
typedef float FloatType;
   * @brief Small value used for numerical differentiation.
 
FloatType delta = 1e-4;
 * @brief Number of epochs for training the neural network.
 
int nepoch = 20;
   * @brief Total number of data points in the training dataset.
 
int ndata = batch_size * nbatch;
 * @brief Training dataset containing input-output pairs.
 
std::vector<XYpair<FloatType,1,1> > data(ndata);
   * @brief Indices of the data points.
 
std::vector<int> didx(ndata);
  
//... 
 * @brief Number of neurons in the hidden layer.
 
int nhidden = 5;
 * @brief Initial weights for the output layer.
 
Matrix<FloatType> winit_out(1,nhidden,0.01);
   * @brief Initial weights for the hidden layer.
 
Matrix<FloatType> winit_h(nhidden,1,0.01);
 * @brief Initial biases for the output layer.
 
Vector<FloatType> binit_out(1,0.01);
   * @brief Initial biases for the hidden layer.
 
Vector<FloatType> binit_h(nhidden, 0.01);
 * @brief Hidden layer of the neural network.
 
auto hidden_layer( dnn_layer(input_layer<FloatType>(), winit_h, binit_h, ReLU<FloatType>()) );
   * @brief Neural network model with mean squared error cost function.
 
auto model = mse_cost( dnn_layer(hidden_layer, winit_out, binit_out) );

//...
 * @brief Learning rate scheduler for the optimizer.
 
DecayScheduler<FloatType> lr(0.001, 0.1);
   * @brief Parameters for the Adam optimizer.
 
AdamParams<FloatType> ap;
   * @brief Adam optimizer with learning rate scheduling.
 
AdamOptimizer<FloatType, DecayScheduler<FloatType> > opt(ap,lr);
   * @brief Trains the neural network model on the training dataset.
 
train(model, data, opt, nepoch, batch_size);

//... 
 * @brief Final parameters of the trained neural network model.
 
Vector<FloatType> final_p = model.getParams();

//... 
 * @brief Average loss of the trained model on the test dataset.
 
FloatType avg_loss= 0.;
  
//...
}* This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testOneHiddenLayer(){
  //Test f(x) = 0.2*x + 0.3;
  int nbatch = 100;
  int batch_size = 4;

  typedef float FloatType;
  FloatType delta = 1e-4;

  int nepoch = 20;
  int ndata = batch_size * nbatch;

  std::vector<XYpair<FloatType,1,1> > data(ndata);
  std::vector<int> didx(ndata);
  
  for(int i=0;i<ndata;i++){ //i = b + batch_size * B
    FloatType eps = 2.0/(ndata - 1);
    FloatType x = -1.0 + i*eps; //normalize x to within +-1

    data[i].x = Vector<FloatType>(1,x);
    data[i].y = Vector<FloatType>(1,0.2*x + 0.3);

    didx[i] = i;
  }

  int nhidden = 5;

  Matrix<FloatType> winit_out(1,nhidden,0.01);
  Matrix<FloatType> winit_h(nhidden,1,0.01);

  Vector<FloatType> binit_out(1,0.01);
  Vector<FloatType> binit_h(nhidden, 0.01);

  auto hidden_layer( dnn_layer(input_layer<FloatType>(), winit_h, binit_h, ReLU<FloatType>()) );
  auto model = mse_cost( dnn_layer(hidden_layer, winit_out, binit_out) );

  //Test derivative
  {
    Vector<FloatType> p = model.getParams();
    
    for(int d=1;d<5;d++){ //first 5 batches (unscrambled)
      auto bxy = batchData(didx.data() + d*batch_size, batch_size, data);
      
      double c1 = model.loss(bxy.x,bxy.y);
      Vector<FloatType> pd = model.deriv();
      
      auto hidden_layer2 = dnn_layer(input_layer<FloatType>(), winit_h, binit_h, ReLU<FloatType>());  
      auto model2 = mse_cost( dnn_layer(hidden_layer2, winit_out, binit_out) );

      std::cout << "Test derivs " << d << " x=" << bxy.x << std::endl;
      for(int i=0;i<p.size(0);i++){
	Vector<FloatType> pp(p);
	doHost(pp, { pp_v(i) += delta; });
	model2.update(pp);
      
	FloatType c2 = model2.loss(bxy.x,bxy.y);
	doHost(pd, {
	    FloatType expect = (c2-c1)/delta;
	    std::cout << i << " got " << pd_v(i) << " expect " << expect << std::endl;
	    if(abs(expect) > 1e-4)
	      assert(abs_near(pd_v(i),expect,FloatType(1e-3)));
	  });
	
      }
    }
  }


  DecayScheduler<FloatType> lr(0.001, 0.1);
  AdamParams<FloatType> ap;
  AdamOptimizer<FloatType, DecayScheduler<FloatType> > opt(ap,lr);
  
  train(model, data, opt, nepoch, batch_size);

  std::cout << "Final params" << std::endl;
  Vector<FloatType> final_p = model.getParams();
  for(int i=0;i<final_p.size(0);i++)
    doHost(final_p, { std::cout << i << " " << final_p_v(i) << std::endl; });

  std::cout << "Test on some data" << std::endl;
  FloatType avg_loss= 0.;
  for(int d=0;d<data.size();d++){ 
    auto got = model.predict(data[d].x);
    std::cout << data[d].x << " got " << got << " expect " << data[d].y << std::endl;
    autoView(got_v,got,HostRead);
    autoView(data_v,data[d].y,HostRead);
    avg_loss += pow(got_v(0) - data_v(0),2);
  }
  avg_loss /= data.size();
  std::cout << "Avg. loss " << avg_loss << std::endl;
  assert(avg_loss < 1e-4);
}

/**
 * @brief Program entry point
 * @param argc Number of command line arguments
 * @param argv Array of command line argument strings
 * @return Exit status of the program
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
int main(int argc, char** argv){
  initialize(argc, argv);
  
  testOneHiddenLayer();
  
  return 0;
}
