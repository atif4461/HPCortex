#include <HPCortex.hpp>
#include <Testing.hpp>

/**
 * @brief Tests simple linear distributed deep learning with data parallelism.
 *
 * This function trains two models, one globally across all ranks and another locally,
 * to verify that both produce similar results under certain conditions.
 
void testSimpleLinearDDP() {
 ...
}
 * @brief Trains a neural network model using gradient descent optimization.
 *
 * @param model The neural network model to be trained.
 * @param data Training dataset.
 * @param opt Optimizer used for training.
 * @param nepoch Number of epochs for training.
 * @param batchSize Batch size for training.
 
train(model, data, opt, nepoch, batchSize); 
 * @brief Returns parameters of the trained model.
 *
 * @return Parameters of the trained model.
 
Vector<FloatType> getParams(); 
 * @brief Checks if two vectors are nearly equal within a specified tolerance.
 *
 * @param v1 First vector to compare.
 * @param v2 Second vector to compare.
 * @param tol Tolerance value for comparison.
 * @param relative Flag indicating whether to perform relative comparison.
 * @return True if vectors are nearly equal, false otherwise.
 
bool near(v1, v2, tol, relative); 
 * @brief Enables or disables data parallelism for distributed deep learning.
 
communicators.enableDDPnoPipelining();
communicators.disableParallelism();
 * @brief Reports setup information for distributed communication.
 
communicators.reportSetup();
 * @brief Gets effective batch size due to data parallelism.
 *
 * @return Effective batch size.
 
int ddpNrank(); 
 * @brief Creates a decay scheduler for learning rate adjustment.
 *
 * @param initialLR Initial learning rate.
 * @param decayRate Learning rate decay factor.
 
DecayScheduler(FloatType)(initialLR, decayRate);
 * @brief Constructs a gradient descent optimizer with a decay scheduler.
 *
 * @param lr Learning rate schedule.
 
GradientDescentOptimizer(opt);
 * @brief Computes mean squared error cost between predicted and actual outputs.
 *
 * @param layer Neural network layer producing output.
 * @return Mean squared error cost.
 
mse_cost(dnn_layer()); 
 * @brief Creates a dense neural network layer.
 *
 * @param inputLayer Input layer to the dense layer.
 * @param weights Weights matrix for the dense layer.
 * @param biases Biases vector for the dense layer.
 * @return Dense neural network layer.
 
dnn_layer(input_layer(), weights, biases);
 * @brief Creates an input layer for the neural network.
 *
 * @return Input layer.
 
input_layer();* This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testSimpleLinearDDP(){
  //Test f(x) = 0.2*x + 0.3;

  typedef float FloatType;
  
  Matrix<FloatType> winit(1,1,0.0);
  Vector<FloatType> binit(1,0.0);

  int nepoch = 10;
  
  int ndata = 100;
  std::vector<XYpair<FloatType,1,1> > data(ndata);
  for(int i=0;i<ndata;i++){
    FloatType eps = 2.0/(ndata - 1);
    FloatType x = -1.0 + i*eps; //normalize x to within +-1

    data[i].x = Vector<FloatType>(1,x);
    data[i].y = Vector<FloatType>(1,0.2*x + 0.3);
  }

  int ddp_eff_batch_size; //with DDP we are effectively increasing the batch size by nrank
  Vector<FloatType> params_global; //params from DDP training 
  {
    communicators().enableDDPnoPipelining();
    communicators().reportSetup();

    ddp_eff_batch_size = communicators().ddpNrank();
    
    auto model = mse_cost( dnn_layer(input_layer<FloatType>(), winit, binit) );
    DecayScheduler<FloatType> lr(0.01, 0.1);
    GradientDescentOptimizer<FloatType, DecayScheduler<FloatType> > opt(lr);
    
    train(model, data, opt, nepoch, 1);
    params_global = model.getParams();
  }
  
  Vector<FloatType> params_local; //params from training on just this rank
  {
    communicators().disableParallelism();
    communicators().reportSetup();
    
    auto model = mse_cost( dnn_layer(input_layer<FloatType>(), winit, binit) );
    DecayScheduler<FloatType> lr(0.01, 0.1);
    GradientDescentOptimizer<FloatType, DecayScheduler<FloatType> > opt(lr);
    
    train(model, data, opt, nepoch, ddp_eff_batch_size);
    params_local = model.getParams();
  }

  std::cout << "Params got " << params_global << " expect " << params_local << std::endl;
  assert(near(params_global, params_local, FloatType(1e-4), true));
}

/**
 * @brief Program entry point
 * @param argc Number of command line arguments
 * @param argv Array of command line argument strings
 * @return Exit status of the program
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
int main(int argc, char** argv){
  initialize(argc, argv);
  
  testSimpleLinearDDP();

  return 0;
}
