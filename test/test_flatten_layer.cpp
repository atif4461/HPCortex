#include <HPCortex.hpp>
#include <Testing.hpp>

/**
 * @brief Tests the functionality of the flatten layer.
 *
 * This function tests the forward and backward passes of the flatten layer,
 * ensuring that the output matches the expected result and that the derivative
 * calculation is correct.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testFlattenLayer(){
  typedef double FloatType; //more precise derivatives
  FloatType delta = 1e-6;

  int dims[3] = {2,3,4};
  typedef std::vector<FloatType> vecD;
  vecD in_lin(2*3*4);
  for(int x=0;x<2;x++)
    for(int y=0;y<3;y++)
      for(int z=0;z<4;z++)
	in_lin[ z + 4*(y + 3*x) ] = 0.1 + 0.01 * (z + 4*(y + 3*x));

  typedef Tensor<FloatType,3> TensType;
  Matrix<FloatType> expect(2*3, 4, in_lin); //will use z as batch_size  
  TensType in(dims, in_lin);

  auto m = flatten_layer( input_layer<FloatType, TensType>() );
  
  Matrix<FloatType> got = m.value(in);
  std::cout << got << std::endl;
  
  assert( abs_near(got, expect, 1e-12, true) );

  //To test the deriv we can just pass in the flattened matrix and should get the tensor back
  //i.e.     dcost/dout_{(i,j), k} = tensflat_{(i,j),k}
  //         dcost/din_{i'j'k'} = dcost/dout_{(i,j), k} dout_{(i,j), k}/din_{i'j'k'}=  tensflat_{(i',j'),k'} = tens_{i'j'k'}
  Vector<FloatType> cost_deriv(0);
  TensType deriv_got;
  m.deriv(cost_deriv,0,std::move(expect), &deriv_got);

  doHost2(in,deriv_got,{
  for(int x=0;x<2;x++)
    for(int y=0;y<3;y++)
      for(int z=0;z<4;z++)
	assert( abs_near(in_v(x,y,z), deriv_got_v(x,y,z), 1e-9 ) );
    });

  std::cout << "Tests passed" << std::endl;
}

/**
 * @brief Tests the functionality of the unflatten layer operation.
 *
 * This function verifies that the unflatten layer correctly transforms 
 * a linearized input vector into its original tensor representation.
 *
 * @details The test case involves creating a sample tensor with known values,
 * then flattening it into a matrix and passing it through the unflatten layer.
 * The resulting output tensor is compared against the expected result.
 * Additionally, the derivative of the unflatten layer is tested by computing 
 * the gradient of the output with respect to the input and verifying its correctness.
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
void testUnflattenLayer(){
  typedef double FloatType; //more precise derivatives
  FloatType delta = 1e-6;

  int dims[3] = {2,3,4};
  typedef std::vector<FloatType> vecD;
  vecD in_lin(2*3*4);
  for(int x=0;x<2;x++)
    for(int y=0;y<3;y++)
      for(int z=0;z<4;z++)
	in_lin[ z + 4*(y + 3*x) ] = 0.1 + 0.01 * (z + 4*(y + 3*x));

  typedef Tensor<FloatType,3> TensType;
  TensType expect(dims, in_lin);
  Matrix<FloatType> in(2*3, 4, in_lin); 

  auto m = unflatten_layer<3>( input_layer<FloatType, Matrix<FloatType> >(), dims );
  
  TensType got = m.value(in);
  
  assert( abs_near(got, expect, 1e-12, true) );

  //To test the deriv we can just pass in the tensor and should get the flattened matrix
  //i.e.   dcost/dout_{i,j,k} = tens_{i,j,k}
  //       dcost/din_{(i',j'),k'} = dcost/dout_{i,j, k} dout_{i,j,k}/din_{(i'j'),k'} = tens_{i',j',k'} = tensflat_{(i',j'),k'}
  Vector<FloatType> cost_deriv(0);
  Matrix<FloatType> deriv_got;
  m.deriv(cost_deriv,0,std::move(expect), &deriv_got);

  doHost2(in,deriv_got,{
  for(int xy=0;xy<2*3;xy++)
    for(int z=0;z<4;z++)
      assert( abs_near(in_v(xy,z), deriv_got_v(xy,z), 1e-9 ) );
    });

  std::cout << "Tests passed" << std::endl;
}


/**
 * @brief Program entry point
 * @param argc Number of command line arguments
 * @param argv Array of command line argument strings
 * @return Exit status of the program
 * This comment was generated by meta-llama/Llama-3.3-70B-Instruct:None at temperature 0.2.
*/ 
int main(int argc, char** argv){
  initialize(argc,argv);
  testFlattenLayer();
  testUnflattenLayer();
  return 0;
}
